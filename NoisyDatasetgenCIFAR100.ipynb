{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1723906795135,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "AUjR-lho0VYd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "from PIL import Image\n",
    "\n",
    "import tools\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19775,
     "status": "ok",
     "timestamp": 1723892057717,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "cpgi5MZm0YVn",
    "outputId": "05881838-f253-44b1-c104-c6b86598bf9a"
   },
   "outputs": [],
   "source": [
    "original_images = np.load('data/cifar100/train_images.npy')\n",
    "original_labels = np.load('data/cifar100/train_labels.npy') \n",
    "data1 = torch.from_numpy(original_images).float().to(device)\n",
    "targets = torch.from_numpy(original_labels).to(device)\n",
    "dataset = zip(data1, targets)\n",
    "data1=original_images\n",
    "# data1=data1.reshape(-1, 3, 32, 32)\n",
    "# data1=data1.transpose((0, 2, 3, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19775,
     "status": "ok",
     "timestamp": 1723892057717,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "cpgi5MZm0YVn",
    "outputId": "05881838-f253-44b1-c104-c6b86598bf9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building dataset...\n"
     ]
    }
   ],
   "source": [
    "noise_rate = 0.2\n",
    "num_classes = 100\n",
    "feature_size = 32*32*3\n",
    "norm_std = 0.1\n",
    "seed = 1\n",
    "new_labels1 = tools.get_instance_noisy_label(noise_rate, dataset, targets, num_classes, feature_size, norm_std, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pickle\n",
    "\n",
    "# def save_cifar_batch(images, labels, batch_label, filename):\n",
    "#     \"\"\"\n",
    "#     Save images and labels in the CIFAR-like format using pickle.\n",
    "\n",
    "#     Args:\n",
    "#     - images: NumPy array of shape (N, 3, 32, 32).\n",
    "#     - labels: List of integer labels.\n",
    "#     - batch_label: String label for the batch.\n",
    "#     - filename: Path where the pickle file will be saved.\n",
    "#     \"\"\"\n",
    "#     # Flatten the images from (N, 3, 32, 32) to (N, 3072)\n",
    "#     flattened_images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "#     # Create a dictionary similar to the CIFAR format\n",
    "#     batch_dict = {\n",
    "#         'data': flattened_images,\n",
    "#         'labels': labels,\n",
    "#         'batch_label': batch_label\n",
    "#     }\n",
    "\n",
    "#     # Save the dictionary as a pickle file\n",
    "#     with open(filename, 'wb') as f:\n",
    "#         pickle.dump(batch_dict, f)\n",
    "\n",
    "# def split_and_save_batches(images, labels, num_batches=5, prefix=\"data_batch_\"):\n",
    "#     \"\"\"\n",
    "#     Split images and labels into several batches and save each as a pickle file.\n",
    "\n",
    "#     Args:\n",
    "#     - images: NumPy array of shape (N, 3, 32, 32).\n",
    "#     - labels: List of integer labels.\n",
    "#     - num_batches: Number of batches to split the data into.\n",
    "#     - prefix: Prefix for the batch file names.\n",
    "#     \"\"\"\n",
    "#     N = images.shape[0]\n",
    "#     batch_size = N // num_batches\n",
    "\n",
    "#     for i in range(num_batches):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = (i + 1) * batch_size if i != num_batches - 1 else N  # Include remaining data in the last batch\n",
    "#         batch_images = images[start_idx:end_idx]\n",
    "#         batch_labels = labels[start_idx:end_idx]\n",
    "#         batch_label = f\"training batch {i+1} of {num_batches}\"\n",
    "\n",
    "#         # Save the batch to a pickle file\n",
    "#         filename = \"./cifar1000.2/\"+f\"{prefix}{i+1}.pkl\"\n",
    "#         save_cifar_batch(batch_images, batch_labels, batch_label, filename)\n",
    "#         print(f\"Saved {filename} with {len(batch_labels)} images.\")\n",
    "\n",
    "\n",
    "\n",
    "# # Split and save into 5 batches\n",
    "# split_and_save_batches(data1, new_labels1, num_batches=5, prefix=\"data_batch_\")\n",
    "\n",
    "# test_images = np.load('data/cifar100/test_images.npy')\n",
    "# test_labels = np.load('data/cifar100/test_labels.npy')\n",
    "# testl = torch.from_numpy(test_labels)\n",
    "\n",
    "\n",
    "# split_and_save_batches(test_images, testl, num_batches=1, prefix=\"test_batch_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data1.reshape(-1, 3, 32, 32)\n",
    "\n",
    "# data1=data1.reshape(-1, 3, 600, 400)\n",
    "data1=data1.transpose((0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r checkpoint\n",
    "# !mkdir checkpoint\n",
    "# !rm -rf ./cifar10/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1723906795135,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "cuzwQBWQ0xT7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tBPggGb5BNyh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=torch.load('./2024_08_14-05_29_56_PM/best_cp/modelk.pth',map_location='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # Load the pretrained ResNet-50 model\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# # Freeze the initial half of the layers\n",
    "# total_layers = len(list(model.children()))  # Total number of layers\n",
    "# freeze_layers = total_layers // 2  # Freeze the first half\n",
    "\n",
    "# for i, child in enumerate(model.children()):\n",
    "#     if i < freeze_layers:\n",
    "#         for param in child.parameters():\n",
    "#             param.requires_grad = False\n",
    "#     else:\n",
    "#         for param in child.parameters():\n",
    "#             param.requires_grad = True\n",
    "# model.to(device)\n",
    "# # Verify which layers are frozen\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f'{name}: {\"Frozen\" if not param.requires_grad else \"Trainable\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision.models as models\n",
    "# import torch.nn.init as init\n",
    "\n",
    "# # Load the pretrained ResNet-50 model\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# # Define a function to apply Xavier initialization\n",
    "# def initialize_weights_xavier(model):\n",
    "#     for m in model.modules():\n",
    "#         if isinstance(m, (nn.Conv2d, nn.Linear)):  # Apply Xavier to Conv2d and Linear layers\n",
    "#             init.xavier_uniform_(m.weight)  # Xavier uniform distribution\n",
    "#             if m.bias is not None:  # If bias exists, initialize it to 0\n",
    "#                 init.constant_(m.bias, 0)\n",
    "\n",
    "# # Apply Xavier initialization to the model\n",
    "# initialize_weights_xavier(model)\n",
    "# model.to(device)\n",
    "# # Check the model to confirm initialization\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tBPggGb5BNyh"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision.models as models\n",
    "\n",
    "\n",
    "# model = models.resnet50(pretrained=True)  \n",
    "\n",
    "\n",
    "# # model.load_state_dict(torch.load('./2024_08_14-05_29_56_PM/best_cp/modelk.pth',map_location='cuda:0'),strict=False)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# model.to(device)\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# # optimizer.load_state_dict(torch.load('./2024_08_14-05_29_56_PM/best_cp/optimizer.pth',map_location='cuda:0'))\n",
    "\n",
    "# # # Initialize Predictor (if it's also a ResNet-18 or another model)\n",
    "# # predictor = models.resnet18(pretrained=False)\n",
    "# # predictor.load_state_dict(torch.load('./2024_08_14-05_29_56_PM/last_cp/predictor.pth'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SFeBO7gpBSAm"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1723906795135,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "ULX5Vrbar6C2"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "class ExampleDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = Image.fromarray(x)\n",
    "        y = self.labels[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)  \n",
    "            \n",
    "        return x, y\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = Image.fromarray(x)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3B-7xmpk-aXK"
   },
   "outputs": [],
   "source": [
    "# dataset = ExampleDataset(data, new_labels,transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "g90OL1_MBWre"
   },
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# criterion1 = nn.CrossEntropyLoss(reduction='none',label_smoothing=0.1)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 2\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, targets in dataloader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         targets=targets.to(device) \n",
    "#         optimizer.zero_grad() \n",
    "\n",
    "#         outputs = model(inputs)  \n",
    "#         loss = criterion(outputs, targets)  \n",
    "\n",
    "#         loss.backward() \n",
    "#         optimizer.step()  \n",
    "#         # scheduler.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stages = [2,3,5]\n",
    "# datasetpred = CustomDataset(data, transform=transform)\n",
    "# dataloaderpred = DataLoader(datasetpred, batch_size=512, shuffle=False)\n",
    "# pseudo_labels=[]\n",
    "# confident_labels=[]\n",
    "# last_epoch=stages[-1]\n",
    "# ep=0;\n",
    "# for st in stages:\n",
    "#     num_epochs = st;\n",
    "#     for epoch in range(ep,num_epochs):\n",
    "#         running_loss = 0.0\n",
    "#         for inputs, targets in dataloader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             targets=targets.to(device) \n",
    "#             optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "#             outputs = model(inputs)  # Forward pass\n",
    "#             loss = criterion(outputs, targets)  # Compute loss\n",
    "\n",
    "#             loss.backward()  # Backward pass\n",
    "#             optimizer.step()  # Optimize\n",
    "#             # scheduler.step()\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "#     all_predictions = []\n",
    "#     reliable_indice=[]\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for inputs in dataloaderpred:\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             all_predictions.extend(predicted.cpu().numpy())\n",
    "#             confidence = torch.softmax(outputs, dim=1).max(dim=1)[0]>0.9\n",
    "#             print(confidence)\n",
    "#             reliable_indice.extend(confidence.cpu().numpy())  # Example threshold\n",
    "\n",
    "#     pseudo_labels.append(np.array(all_predictions))\n",
    "#     confident_labels.append(np.array(reliable_indice))\n",
    "#     ep=st;\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stages = [2,3,5]\n",
    "# datasetpred = ExampleDataset(data, new_labels,transform=transform)\n",
    "# dataloaderpred = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "# pseudo_labels=[]\n",
    "# confident_labels=[]\n",
    "# last_epoch=stages[-1]\n",
    "# ep=0;\n",
    "# for st in stages:\n",
    "#     num_epochs = st;\n",
    "#     for epoch in range(ep,num_epochs):\n",
    "#         running_loss = 0.0\n",
    "#         for inputs, targets in dataloader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             targets=targets.to(device) \n",
    "#             optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "#             outputs = model(inputs)  # Forward pass\n",
    "#             loss = criterion(outputs, targets)  # Compute loss\n",
    "\n",
    "#             loss.backward()  # Backward pass\n",
    "#             optimizer.step()  # Optimize\n",
    "#             # scheduler.step()\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "#     all_predictions = []\n",
    "#     reliable_indice=[]\n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in dataloaderpred: \n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            \n",
    "#             # Forward pass to get outputs\n",
    "#             outputs = model(inputs)\n",
    "            \n",
    "#             # Get the predictions\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             all_predictions.extend(predicted.cpu().numpy())\n",
    "            \n",
    "#             # Calculate per-sample loss\n",
    "#             loss_per_sample = criterion1(outputs, labels) \n",
    "\n",
    "#             # print(loss_per_sample)\n",
    "#             reliable_indices = loss_per_sample < 1  # Example threshold for loss\n",
    "#             # print(reliable_indices)\n",
    "#             # Extend reliable indices list with boolean values indicating reliability\n",
    "#             reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "    \n",
    "#     pseudo_labels.append(np.array(all_predictions))\n",
    "#     confident_labels.append(np.array(reliable_indice))\n",
    "#     ep=st;\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df=pd.DataFrame(pseudo_labels)\n",
    "# df.to_csv(\"Iteration1_Labels_235\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_elements_same_index(arrays):\n",
    "    if not arrays:\n",
    "        return []\n",
    "\n",
    "    # Number of arrays\n",
    "    num_arrays = len(arrays)\n",
    "    \n",
    "    # Length of the first array\n",
    "    num_elements = len(arrays[0])\n",
    "\n",
    "    # Store indices where all arrays have the same element\n",
    "    common_indices = []\n",
    "    \n",
    "    # Iterate over each index\n",
    "    for i in range(num_elements):\n",
    "        if all(array[i] == 1 for array in arrays):\n",
    "            common_indices.append(i)\n",
    "    \n",
    "    return common_indices\n",
    "\n",
    "\n",
    "# result = common_elements_same_index(confident_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def common_elements_same_index(arrays,labels):\n",
    "#     if not arrays:\n",
    "#         return []\n",
    "\n",
    "#     # Number of arrays\n",
    "#     num_arrays = len(arrays)\n",
    "    \n",
    "#     # Length of the first array\n",
    "#     num_elements = len(arrays[0])\n",
    "\n",
    "#     # Store indices where all arrays have the same element\n",
    "#     common_indices = []\n",
    "    \n",
    "#     # Iterate over each index\n",
    "#     for i in range(num_elements):\n",
    "#         # Get the element at index i in the first array\n",
    "#         element = arrays[0][i]\n",
    "        \n",
    "#         # Check if all arrays have the same element at index i\n",
    "#         if all(array[i] == element for array in arrays):\n",
    "#             if all(label[i] == 1 for label in labels):\n",
    "#                 common_indices.append(i)\n",
    "    \n",
    "#     return common_indices\n",
    "\n",
    "\n",
    "# result = common_elements_same_index(pseudo_labels,confident_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itr1_pseudo=new_labels\n",
    "def replace_elements_at_indices(another_array, common_indices, arrays):\n",
    "    k=len(arrays)\n",
    "    for i in common_indices:\n",
    "        another_array[i] = arrays[k-1][i]\n",
    "    \n",
    "    return another_array\n",
    "\n",
    "# itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result, pseudo_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Unfreeze all layers for further fine-tuning\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True  # Unfreeze all parameters\n",
    "\n",
    "# # Redefine the optimizer to include all parameters now that all layers are unfrozen\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)  # Use a smaller learning rate for fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p=[]\n",
    "# a=[]\n",
    "# for i in range(2):\n",
    "#     print(f'Iteration: {i}')\n",
    "#     stages = [2,4,6]\n",
    "#     datasetpred = CustomDataset(data, transform=transform)\n",
    "#     dataloaderpred = DataLoader(datasetpred, batch_size=512, shuffle=False)\n",
    "#     pseudo_labels1=[]\n",
    "#     confident_labels=[]\n",
    "#     last_epoch=stages[-1]\n",
    "#     ep=0;\n",
    "#     for st in stages:\n",
    "#         num_epochs = st;\n",
    "#         for epoch in range(ep,num_epochs):\n",
    "#             running_loss = 0.0\n",
    "#             for inputs, targets in dataloader:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 targets=targets.to(device) \n",
    "#                 optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "#                 outputs = model(inputs)  # Forward pass\n",
    "#                 loss = criterion(outputs, targets)  # Compute loss\n",
    "\n",
    "#                 loss.backward()  # Backward pass\n",
    "#                 optimizer.step()  # Optimize\n",
    "#                 # scheduler.step()\n",
    "#                 running_loss += loss.item()\n",
    "\n",
    "#             print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "#         all_predictions = []\n",
    "#         reliable_indice=[]\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for inputs in dataloaderpred:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 all_predictions.extend(predicted.cpu().numpy())\n",
    "#                 confidence = torch.softmax(outputs, dim=1).max(dim=1)[0]>0.9\n",
    "#                 reliable_indice.extend(confidence.cpu().numpy())  # Example threshold\n",
    "\n",
    "#         pseudo_labels1.append(np.array(all_predictions))\n",
    "#         confident_labels.append(np.array(reliable_indice))\n",
    "#         ep=st;\n",
    "#     result1 = common_elements_same_index(pseudo_labels1,confident_labels)\n",
    "#     p.append(result1)\n",
    "#     a.append(pseudo_labels1)\n",
    "#     print(len(result1))\n",
    "#     itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result1, pseudo_labels1)\n",
    "#     dataset = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "#     dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "    \n",
    "    \n",
    "# for i in range(4):\n",
    "#     print(f'Iteration: {2+i}')\n",
    "#     stages = [3,6,10]\n",
    "#     datasetpred = CustomDataset(data, transform=transform)\n",
    "#     dataloaderpred = DataLoader(datasetpred, batch_size=512, shuffle=False)\n",
    "#     pseudo_labels1=[]\n",
    "#     confident_labels=[]\n",
    "#     last_epoch=stages[-1]\n",
    "#     ep=0;\n",
    "#     for st in stages:\n",
    "#         num_epochs = st;\n",
    "#         for epoch in range(ep,num_epochs):\n",
    "#             running_loss = 0.0\n",
    "#             for inputs, targets in dataloader:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 targets=targets.to(device) \n",
    "#                 optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "#                 outputs = model(inputs)  # Forward pass\n",
    "#                 loss = criterion(outputs, targets)  # Compute loss\n",
    "\n",
    "#                 loss.backward()  # Backward pass\n",
    "#                 optimizer.step()  # Optimize\n",
    "#                 # scheduler.step()\n",
    "#                 running_loss += loss.item()\n",
    "\n",
    "#             print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "#         all_predictions = []\n",
    "#         reliable_indice=[]\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for inputs in dataloaderpred:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 all_predictions.extend(predicted.cpu().numpy())\n",
    "#                 confidence = torch.softmax(outputs, dim=1).max(dim=1)[0]>0.9\n",
    "#                 reliable_indice.extend(confidence.cpu().numpy())  # Example threshold\n",
    "\n",
    "#         pseudo_labels1.append(np.array(all_predictions))\n",
    "#         confident_labels.append(np.array(reliable_indice))\n",
    "#         ep=st;\n",
    "#     result1 = common_elements_same_index(pseudo_labels1,confident_labels)\n",
    "#     p.append(result1)\n",
    "#     a.append(pseudo_labels1)\n",
    "#     print(len(result1))\n",
    "#     itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result1, pseudo_labels1)\n",
    "#     dataset = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "#     dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 4\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Data Augmentation (BYOL Transform for CIFAR-10)\n",
    "class BYOLTransform:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=32),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.transform(x), self.transform(x)\n",
    "\n",
    "# MLP for projection and prediction head\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=4096, out_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# BYOL model with backbone, projector, and predictor\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(self, base_encoder, out_dim=256, momentum=0.996):\n",
    "        super(BYOL, self).__init__()\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Online network\n",
    "        self.online_encoder = base_encoder\n",
    "        num_ftrs = self.online_encoder.fc.in_features\n",
    "        self.online_encoder.fc = MLP(num_ftrs, out_dim=out_dim)\n",
    "        \n",
    "        # Target network (initialized with online network weights)\n",
    "        self.target_encoder = base_encoder\n",
    "        self.target_encoder.load_state_dict(self.online_encoder.state_dict())\n",
    "        self.target_encoder.fc = MLP(num_ftrs, out_dim=out_dim)\n",
    "        \n",
    "        # Predictor\n",
    "        self.predictor = MLP(out_dim, out_dim=out_dim)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_target_encoder(self):\n",
    "        # Momentum update for the target encoder\n",
    "        for online_params, target_params in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):\n",
    "            target_params.data = self.momentum * target_params.data + (1.0 - self.momentum) * online_params.data\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Online network forward pass\n",
    "        online_proj_1 = self.online_encoder(x1)\n",
    "        online_proj_2 = self.online_encoder(x2)\n",
    "        \n",
    "        # Predict from online projection\n",
    "        pred_1 = self.predictor(online_proj_1)\n",
    "        pred_2 = self.predictor(online_proj_2)\n",
    "        \n",
    "        # Target network forward pass (no gradient computation)\n",
    "        with torch.no_grad():\n",
    "            self._momentum_update_target_encoder()  # update the target encoder\n",
    "            target_proj_1 = self.target_encoder(x1)\n",
    "            target_proj_2 = self.target_encoder(x2)\n",
    "        \n",
    "        return pred_1, pred_2, target_proj_1, target_proj_2\n",
    "\n",
    "# Loss function (mean squared error between predictions and targets)\n",
    "def byol_loss_fn(pred, target):\n",
    "    pred = F.normalize(pred, dim=-1, p=2)\n",
    "    target = F.normalize(target, dim=-1, p=2)\n",
    "    return 2 - 2 * (pred * target).sum(dim=-1).mean()\n",
    "\n",
    "# CIFAR-10 Dataset class\n",
    "class CIFAR10BYOLDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = Image.fromarray(x)\n",
    "        y = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x_i, x_j = self.transform(x)\n",
    "        else:\n",
    "            x_i, x_j = x, x\n",
    "\n",
    "        return (x_i, x_j), y\n",
    "\n",
    "# Training function for BYOL with CIFAR-10\n",
    "def train_byol(data_loader, model, optimizer, device='cuda'):\n",
    "    model.train()\n",
    "    for (x_i, x_j), _ in data_loader:\n",
    "        x_i = x_i.to(device)\n",
    "        x_j = x_j.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_1, pred_2, target_1, target_2 = model(x_i, x_j)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_1 = byol_loss_fn(pred_1, target_2)\n",
    "        loss_2 = byol_loss_fn(pred_2, target_1)\n",
    "        loss = (loss_1 + loss_2) / 2\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, pretrained_backbone, out_dim=10):\n",
    "        super(FineTunedModel, self).__init__()\n",
    "        self.backbone = pretrained_backbone  # Pretrained SSL backbone (ResNet)\n",
    "        \n",
    "        # Freeze the backbone parameters (optional, can be unfrozen later for fine-tuning)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # New classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(),               # ReLU activation\n",
    "            nn.Dropout(0.3),          # Dropout with 30% probability\n",
    "            nn.Linear(256, out_dim)   # Classification layer with output size 10 (for CIFAR-10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)       # Pass input through the pretrained backbone\n",
    "        x = self.classifier(x)     # Pass through the classifier head\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Augmentation (SimCLR Transform)\n",
    "# class SimCLRTransform:\n",
    "#     def __init__(self, size=224):\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.RandomResizedCrop(size=size),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "#             transforms.RandomGrayscale(p=0.2),\n",
    "#             transforms.GaussianBlur(kernel_size=9),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "#         ])\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         # Apply the transformation twice to create two views\n",
    "#         return self.transform(x), self.transform(x)\n",
    "\n",
    "\n",
    "SimCLRTransform=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=16),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=9),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "        ])\n",
    "\n",
    "SimCLRTransform1=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=64),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=9),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "        ])\n",
    "\n",
    "# ResNet Backbone with Projection Head\n",
    "class ResNetSimCLR(nn.Module):\n",
    "    def __init__(self, out_dim=128):\n",
    "        super(ResNetSimCLR, self).__init__()\n",
    "        # Load a pre-trained ResNet-50 model\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Remove the classification layer\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # Projection head: 2-layer MLP\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.projector(x)\n",
    "        return x\n",
    "\n",
    "# NT-Xent Loss (Contrastive Loss)\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        # Normalize the projections\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "        # Concatenate both projections\n",
    "        batch_size = z_i.shape[0]\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "\n",
    "        # Compute cosine similarity matrix\n",
    "        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature\n",
    "\n",
    "        # Create labels for positive pairs\n",
    "        labels = torch.cat([torch.arange(batch_size) for _ in range(2)], dim=0).to(z_i.device)\n",
    "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "\n",
    "        # Mask out the diagonal (self-similarity)\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(z_i.device)\n",
    "        labels = labels[~mask].view(labels.shape[0], -1)\n",
    "        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
    "\n",
    "        # Compute loss\n",
    "        positives = similarity_matrix[labels.bool()].view(batch_size, -1)\n",
    "        negatives = similarity_matrix[~labels.bool()].view(batch_size, -1)\n",
    "\n",
    "        logits = torch.cat([positives, negatives], dim=1)\n",
    "        targets = torch.zeros(logits.shape[0], dtype=torch.long).to(z_i.device)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "    \n",
    "class ExampleDataset1(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the original image\n",
    "        x = self.data[index]\n",
    "        x = Image.fromarray(x)\n",
    "        y = self.labels[index]\n",
    "\n",
    "        # Apply the SimCLR transformations (two different augmented views of the same image)\n",
    "        if self.transform:\n",
    "            x_i = self.transform(x)  # First view\n",
    "            x_j = self.transform(x)  # Second view\n",
    "        else:\n",
    "            x_i, x_j = x, x  # In case no transformation is applied\n",
    "\n",
    "        return (x_i, x_j), y  # Return both views and the label (y) if applicable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.load('data/cifar100/test_images.npy')\n",
    "test_labels = np.load('data/cifar100/test_labels.npy')\n",
    "from sklearn.model_selection import train_test_split\n",
    "# test_dataset = ExampleDataset(test_images, test_labels,transform)\n",
    "test_images=test_images.reshape(-1, 3, 32, 32)\n",
    "datatest=test_images\n",
    "datatest=datatest.reshape(-1, 3, 32, 32)\n",
    "datatest=datatest.transpose((0, 2, 3, 1))\n",
    "testl = torch.from_numpy(test_labels)\n",
    "\n",
    "val_images, test_images, vallabels, test_labels = train_test_split(\n",
    "    datatest, testl, test_size=0.4, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=32, scale=(0.8, 1.0)),  # Resize and crop\n",
    "    transforms.RandomHorizontalFlip(),  # Horizontal flip\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),  # Color jitter\n",
    "    transforms.RandomGrayscale(p=0.2),  # Convert to grayscale randomly\n",
    "#     transforms.GaussianBlur(kernel_size=3),  # Apply Gaussian blur\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for fold, (train_idx, val_idx) in enumerate(kf.split(data1)):\n",
    "# from PreResNet import *\n",
    "\n",
    "# def create_model():\n",
    "#     model = ResNet34(num_classes=100)\n",
    "#     model = model.cuda()\n",
    "#     return model\n",
    "# #     data = data1[train_idx]\n",
    "# #     new_labels = new_labels1[train_idx]\n",
    "# #     val_data = data1[val_idx]\n",
    "# #     val_labels = targets[val_idx]\n",
    "# fold = 0\n",
    "\n",
    "# data = data1\n",
    "# new_labels = new_labels1\n",
    "# val_data = val_images\n",
    "# val_labels = vallabels\n",
    "\n",
    "# #     model = models.resnet50(pretrained=True)  \n",
    "# # # model.load_state_dict(torch.load('./2024_08_14-05_29_56_PM/best_cp/modelk.pth',map_location='cuda:0'),strict=False)\n",
    "\n",
    "\n",
    "# #     model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# # model.to(device)\n",
    "# # model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# # model.to(device)\n",
    "\n",
    "\n",
    "# # criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# # criterion1 = nn.CrossEntropyLoss(reduction='none',label_smoothing=0.1)\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # Model, Loss, and Optimizer\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# # resnet = models.resnet18(pretrained=False)\n",
    "# resnet = create_model()\n",
    "# # resnet.load_state_dict(torch.load('net1withoutssl0.5.pth'))\n",
    "# # num_ftrs = resnet.linear.in_features\n",
    "# # resnet.linear = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "# # model = nn.Sequential(\n",
    "# #     resnet,                    # ResNet-50 backbone without classification layer\n",
    "# #     nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "# #     nn.ReLU(),                  # ReLU activation function\n",
    "# #     nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    "# # )\n",
    "# model=resnet\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # num_ftrs = resnet.fc.in_features\n",
    "# # resnet.fc = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "# # model = nn.Sequential(\n",
    "# #     resnet,                    # ResNet-50 backbone without classification layer\n",
    "# #     nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "# #     nn.ReLU(),                  # ReLU activation function\n",
    "# #     nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    "# # )\n",
    "# # model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# # model = models.resnet50(pretrained=False)\n",
    "# # model.load_state_dict(torch.load('resnet50_weights.pth'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# criterion = NTXentLoss(temperature=0.3)\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "# dataset = ExampleDataset1(data, new_labels,transform=SimCLRTransform)\n",
    "# dataloader = DataLoader(dataset, batch_size=400, shuffle=True,num_workers=4)\n",
    "# epochs = 15\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, _ in dataloader:\n",
    "#         img_1, img_2 = images  # Two augmented views of the same image\n",
    "\n",
    "#         # Move to device\n",
    "#         img_1, img_2 = img_1.to(device), img_2.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         z_i = model(img_1)\n",
    "#         z_j = model(img_2)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(z_i, z_j)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    \n",
    "    \n",
    "# dataset = ExampleDataset1(data, new_labels,transform=SimCLRTransform1)\n",
    "# dataloader = DataLoader(dataset, batch_size=200, shuffle=True,num_workers=4)\n",
    "# epochs = 15\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, _ in dataloader:\n",
    "#         img_1, img_2 = images  # Two augmented views of the same image\n",
    "\n",
    "#         # Move to device\n",
    "#         img_1, img_2 = img_1.to(device), img_2.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         z_i = model(img_1)\n",
    "#         z_j = model(img_2)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(z_i, z_j)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# torch.save(model.state_dict(), 'SSLcifar100Resnet34.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.alpha = alpha  # Weighting factor for class imbalance\n",
    "#         self.gamma = gamma  # Focusing parameter\n",
    "#         self.reduction = reduction\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "#         ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")  # Compute cross-entropy loss\n",
    "#         pt = torch.exp(-ce_loss)  # Probabilities of correct class\n",
    "#         focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss  # Apply focal weight\n",
    "        \n",
    "#         if self.reduction == 'mean':\n",
    "#             return focal_loss.mean()\n",
    "#         elif self.reduction == 'sum':\n",
    "#             return focal_loss.sum()\n",
    "#         else:\n",
    "#             return focal_loss  # No reduction\n",
    "\n",
    "# # Example usage\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NCE_MAE_Loss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha (float): Weight balancing NCE and MAE loss.\n",
    "            reduction (str): 'mean' for average loss, 'none' for element-wise loss.\n",
    "        \"\"\"\n",
    "        super(NCE_MAE_Loss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def nce_loss(self, pred_prob, true_prob):\n",
    "        \"\"\"\n",
    "        Computes Normalized Cross-Entropy (NCE) loss.\n",
    "        \"\"\"\n",
    "        num = -torch.sum(true_prob * torch.log(pred_prob + 1e-9), dim=1)  # Avoid log(0)\n",
    "        denom = -torch.sum(true_prob, dim=1) * torch.sum(torch.log(pred_prob + 1e-9), dim=1)\n",
    "        loss = num / (denom + 1e-9)  # Avoid division by zero\n",
    "\n",
    "        return loss if self.reduction == 'none' else loss.mean()\n",
    "\n",
    "    def mae_loss(self, pred_prob, true_prob):\n",
    "        \"\"\"\n",
    "        Computes Mean Absolute Error (MAE) loss.\n",
    "        \"\"\"\n",
    "        loss = torch.sum(torch.abs(pred_prob - true_prob), dim=1)\n",
    "        return loss if self.reduction == 'none' else loss.mean()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Computes combined NCE+MAE loss.\n",
    "        Args:\n",
    "            outputs (Tensor): Raw logits from model (shape: [batch_size, num_classes])\n",
    "            targets (Tensor): Class indices (shape: [batch_size])\n",
    "        Returns:\n",
    "            Tensor: Combined loss.\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities\n",
    "        pred_prob = F.softmax(outputs, dim=1)  \n",
    "\n",
    "        # Convert class indices to one-hot encoding\n",
    "        num_classes = outputs.shape[1]\n",
    "        true_prob = F.one_hot(targets, num_classes=num_classes).float()\n",
    "\n",
    "        # Compute losses\n",
    "        loss_nce = self.nce_loss(pred_prob, true_prob)\n",
    "        loss_mae = self.mae_loss(pred_prob, true_prob)\n",
    "\n",
    "        # Combine losses\n",
    "        combined_loss =  loss_nce + loss_mae\n",
    "        return combined_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class NDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None, num_classes=100):\n",
    "        \"\"\"\n",
    "        A dataset class that maintains label confidence scores for noisy datasets.\n",
    "\n",
    "        Args:\n",
    "            images (list or numpy.ndarray): The dataset of images.\n",
    "            labels (list or numpy.ndarray): The noisy labels corresponding to the images.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "            num_classes (int, optional): Number of classes in the dataset (default is 10).\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = np.array(labels)\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Initialize label confidence scores uniformly\n",
    "        self.label_confidence = torch.ones(len(self.labels), self.num_classes) / self.num_classes\n",
    "\n",
    "    def update_confidence(self, model, device, m=0, batch_size=512, num_workers=4):\n",
    "        \"\"\"\n",
    "        Updates label confidence scores based on the given mathematical expression.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The trained model.\n",
    "            device (torch.device): The device to run the model on (e.g., 'cpu' or 'cuda').\n",
    "            criterion: Loss function to compute L(f(x; ), y).\n",
    "            m (float): A variable that affects the size of |Dce|.\n",
    "            batch_size (int): The number of images per batch.\n",
    "            num_workers (int): Number of subprocesses for data loading (default: 4).\n",
    "        \"\"\"\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        dataset = ExampleDataset(self.images, self.labels,self.transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1, reduction='none')\n",
    "        \n",
    "\n",
    "#         total_loss = 0.0\n",
    "#         num_samples = 0\n",
    "#         losses = []\n",
    "\n",
    "#         # Compute losses for the entire dataset\n",
    "#         with torch.no_grad():\n",
    "#             for batch_images, batch_labels in data_loader:\n",
    "#                 batch_images = batch_images.to(device)\n",
    "#                 batch_labels = batch_labels.to(device)\n",
    "\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(batch_images)\n",
    "# #                     batch_labels = torch.tensor(self.labels[num_samples:num_samples + batch_images.size(0)], device=device)\n",
    "\n",
    "#                 # Compute loss\n",
    "#                 loss = criterion(outputs, batch_labels)\n",
    "#                 losses.append(loss)\n",
    "#                 total_loss += loss.sum().item()\n",
    "#                 num_samples += batch_images.size(0)\n",
    "\n",
    "#         # Compute mean loss \n",
    "#         mu = total_loss / num_samples\n",
    "\n",
    "        # Update confidence scores using the formula\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (batch_images,batch_labels) in enumerate(data_loader):\n",
    "                batch_images = batch_images.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_images)\n",
    "#                     batch_labels = torch.tensor(self.labels[batch_idx * batch_size : batch_idx * batch_size + batch_images.size(0)], device=device)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                # Compute loss\n",
    "#                 loss = criterion(outputs, batch_labels)\n",
    "\n",
    "#                 # Compute h(f(x;)y) using the given formula\n",
    "#                 h_values = torch.sigmoid(0.5 * (-loss + mu + m))\n",
    "\n",
    "                # Update confidence scores\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_images.size(0)\n",
    "                self.label_confidence[start_idx:end_idx] = probs.cpu()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the image, label, and confidence scores at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the data point.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, label, confidence_scores)\n",
    "        \"\"\"\n",
    "        image = Image.fromarray(self.images[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        confidence = self.label_confidence[idx]\n",
    "        return image, label, confidence\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Define Training Loss Function with Confidence Incorporation\n",
    "class ConfidenceWeightedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConfidenceWeightedLoss, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "\n",
    "    def forward(self, outputs, targets, confidences):\n",
    "        losses = self.criterion(outputs, targets)\n",
    "#         log_probs = F.log_softmax(outputs, dim=1)\n",
    "#         Deno = torch.sum(log_probs, dim=1)\n",
    "#         losses1=-losses/(Deno+1e-8)\n",
    "        weighted_loss = (losses * confidences[range(len(targets)), targets]).mean()\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def dynamic_loss_thresholding(losses, labels, percentile=30):\n",
    "    \"\"\"Computes class-wise dynamic loss threshold based on the given percentile while preserving order.\"\"\"\n",
    "    thresholds = {}\n",
    "    reliable_indices = torch.zeros_like(losses, dtype=torch.bool)\n",
    "    \n",
    "    unique_classes = torch.unique(labels)\n",
    "    for cls in unique_classes:\n",
    "        class_mask = labels == cls\n",
    "        class_losses = losses[class_mask]\n",
    "        if len(class_losses) > 0:\n",
    "            threshold = np.percentile(class_losses.cpu().numpy(), percentile)\n",
    "            thresholds[cls.item()] = threshold\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        cls = labels[i].item()\n",
    "        if cls in thresholds and losses[i] < thresholds[cls]:\n",
    "            reliable_indices[i] = True\n",
    "    \n",
    "    return reliable_indices\n",
    "\n",
    "def pseudo_labeling(model, dataloader, criterion1, device, percentile=30):\n",
    "    \"\"\"Generates pseudo-labels and selects reliable samples using dynamic loss thresholding while preserving order.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_losses = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            loss_per_sample = criterion1(outputs, labels)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_losses.extend(loss_per_sample.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_losses = torch.tensor(all_losses, device=device)\n",
    "    all_labels = torch.tensor(all_labels, device=device)\n",
    "    \n",
    "    reliable_indices = dynamic_loss_thresholding(all_losses, all_labels, percentile)\n",
    "    reliable_indices_np = reliable_indices.cpu().numpy().astype(int)  # Convert to binary 1/0\n",
    "    \n",
    "    return all_predictions, reliable_indices_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0459\n",
      "Epoch [2/5], Loss: 0.0460\n",
      "Epoch [3/5], Loss: 0.0461\n",
      "Epoch [4/5], Loss: 0.0463\n",
      "Epoch [5/5], Loss: 0.0464\n",
      "Fold 1 F1 Score: 0.0069\n",
      "Epoch [1/4], Loss: 0.0241\n",
      "Epoch [2/4], Loss: 0.7175\n",
      "Epoch [3/4], Loss: 0.1506\n",
      "Epoch [4/4], Loss: 0.1190\n",
      "Fold 1 F1 Score: 0.7139\n",
      "27015\n",
      "Iteration: 0\n",
      "Epoch [1/7], Loss: 0.0514\n",
      "Epoch [2/7], Loss: 0.0601\n",
      "Epoch [3/7], Loss: 0.0595\n",
      "Epoch [4/7], Loss: 0.0555\n",
      "Epoch [5/7], Loss: 0.0526\n",
      "Epoch [6/7], Loss: 0.0476\n",
      "Epoch [7/7], Loss: 0.0452\n",
      "56890\n",
      "Fold 1 F1 Score: 0.7704\n",
      "Iteration: 1\n",
      "Epoch [1/7], Loss: 0.0774\n",
      "Epoch [2/7], Loss: 0.0691\n",
      "Epoch [3/7], Loss: 0.0652\n",
      "Epoch [4/7], Loss: 0.0571\n",
      "Epoch [5/7], Loss: 0.0567\n",
      "Epoch [6/7], Loss: 0.0521\n",
      "Epoch [7/7], Loss: 0.0480\n",
      "109411\n",
      "Fold 1 F1 Score: 0.7746\n",
      "Iteration: 2\n",
      "Epoch [1/7], Loss: 0.0621\n",
      "Epoch [2/7], Loss: 0.0628\n",
      "Epoch [3/7], Loss: 0.0621\n",
      "Epoch [4/7], Loss: 0.0605\n",
      "Epoch [5/7], Loss: 0.0578\n",
      "Epoch [6/7], Loss: 0.0568\n",
      "Epoch [7/7], Loss: 0.0561\n",
      "208878\n",
      "Fold 1 F1 Score: 0.7740\n"
     ]
    }
   ],
   "source": [
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(data1)):\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from PreResNet import *\n",
    "dmp=1\n",
    "def create_model():\n",
    "    model = ResNet34(num_classes=100)\n",
    "    model = model.cuda()\n",
    "    return model\n",
    "\n",
    "fold = 0\n",
    "\n",
    "data = data1\n",
    "new_labels = new_labels1\n",
    "val_data = val_images\n",
    "val_labels = vallabels\n",
    "\n",
    "\n",
    "\n",
    "# # Model, Loss, and Optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# resnet = models.resnet18(pretrained=False)\n",
    "resnet = create_model()\n",
    "# resnet.load_state_dict(torch.load('net1cifar1000.5withoutssl.pth'))\n",
    "# resnet.load_state_dict(torch.load('SSLcifar100_0.2.pth'))\n",
    "# resnet.load_state_dict(torch.load('SSLcifar100_0.2.pth'))\n",
    "resnet.load_state_dict(torch.load('net1SSLcifar1000.2resnet34.pth'))\n",
    "# resnet.load_state_dict(torch.load('sslthendividemix.pth'))\n",
    "\n",
    "num_ftrs = resnet.linear.in_features\n",
    "resnet.linear = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "model = nn.Sequential(\n",
    "    resnet,                    # ResNet-50 backbone without classification layer\n",
    "    nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "    nn.ReLU(),                  # ReLU activation function\n",
    "    nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#simclr\n",
    "model = nn.Sequential(\n",
    "    model,                    # ResNet-50 backbone without classification layer\n",
    "    nn.ReLU(),                  # ReLU activation function\n",
    "#     nn.Dropout(0.7),\n",
    "    nn.Linear(128, 100)         # Second linear layer projecting to 128 dimensions\n",
    ")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# criterion1 = nn.CrossEntropyLoss(reduction='none')\n",
    "training_criterion = ConfidenceWeightedLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001,weight_decay=0.03)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.95,weight_decay=5e-4)\n",
    "criterion1 = NCE_MAE_Loss(reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = NDataset(data, new_labels,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True,num_workers=4)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define optimizer\n",
    "# initial_lr = 1e-5\n",
    "# final_lr = 1e-4\n",
    "# # Define a linear warm-up schedule\n",
    "# lambda_lr = lambda epoch: 1 + (final_lr - initial_lr) * (epoch / (num_epochs - 1)) / initial_lr\n",
    "# scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    dataset.update_confidence(model, device)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets, confidences in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets=targets.to(device)\n",
    "        confidences=confidences.to(device)\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs = model(inputs)  \n",
    "#         loss = criterion(outputs, targets)\n",
    "#         confidences = torch.softmax(outputs, dim=1)\n",
    "        loss = training_criterion(outputs, targets, confidences)\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step()  \n",
    "#         scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "\n",
    "\n",
    "val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# model.train()\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.95)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "stages = [2,3,4]\n",
    "# stages=[2,4,7,9,11]\n",
    "datasetpred = ExampleDataset(data, new_labels,transform=transform)\n",
    "dataloaderpred = DataLoader(datasetpred, batch_size=512, shuffle=False,num_workers=4)\n",
    "pseudo_labels=[]\n",
    "confident_labels=[]\n",
    "last_epoch=stages[-1]\n",
    "ep=0;\n",
    "model.train()\n",
    "for st in stages:\n",
    "    num_epochs = st;\n",
    "    for epoch in range(ep,num_epochs):\n",
    "        dataset.update_confidence(model, device)\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets, confidences in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets=targets.to(device) \n",
    "            confidences=confidences.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "#             loss = criterion(outputs, targets)  # Compute loss\n",
    "#             confidences = torch.softmax(outputs, dim=1)\n",
    "            loss = training_criterion(outputs, targets, confidences)\n",
    "\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize\n",
    "            # scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "    all_predictions = []\n",
    "    reliable_indice=[]\n",
    "    model.eval()\n",
    "#     pred, reliable_indices = pseudo_labeling(model, dataloaderpred, criterion1, device)\n",
    "#     pseudo_labels.append(pred)\n",
    "#     confident_labels.append(reliable_indices)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaderpred: \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Calculate per-sample loss\n",
    "            loss_per_sample = criterion1(outputs, labels) \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # print(loss_per_sample)\n",
    "            reliable_indices = loss_per_sample < 1  # Example threshold for loss\n",
    "            # print(reliable_indices)\n",
    "            # Extend reliable indices list with boolean values indicating reliability\n",
    "            reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "\n",
    "    pseudo_labels.append(np.array(all_predictions))\n",
    "    confident_labels.append(np.array(reliable_indice))\n",
    "    ep=st;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# model.train()\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "result = common_elements_same_index(confident_labels)\n",
    "print(len(result))\n",
    "itr1_pseudo=new_labels\n",
    "itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result, pseudo_labels)\n",
    "\n",
    "augmented_data = []\n",
    "augmented_labels = []\n",
    "for idx in result:\n",
    "    input_data = data[idx]  # Get the data for the current index\n",
    "    input_data = Image.fromarray(input_data)\n",
    "    for _ in range(dmp):  # Perform 5 augmentations\n",
    "        augmented_input = augmentation_transforms(input_data)  # Apply augmentations\n",
    "        augmented_input = (augmented_input.permute(1, 2, 0).numpy()*255).astype(np.uint8)\n",
    "        augmented_data.append(augmented_input)\n",
    "        augmented_labels.append(itr1_pseudo[idx])  # Use the pseudo-label for augmented data\n",
    "\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Concatenate the original data and augmented data\n",
    "data = np.concatenate((data, np.array(augmented_data)), axis=0)\n",
    "itr1_pseudo = np.concatenate((itr1_pseudo, augmented_labels), axis=0)\n",
    "\n",
    "\n",
    "dataset = NDataset(data, itr1_pseudo,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True,num_workers=4)\n",
    "datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "dataloaderpred = DataLoader(datasetpred, batch_size=512, shuffle=False,num_workers=4)\n",
    "p=[]\n",
    "a=[]\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "augmented_epochs=[1,3,5]\n",
    "for i in range(3):\n",
    "    print(f'Iteration: {i}')\n",
    "    stages = [2,5,7]\n",
    "#     stages = [3,5,6,8,10]\n",
    "    pseudo_labels1=[]\n",
    "    confident_labels=[]\n",
    "    last_epoch=stages[-1]\n",
    "    ep=0;\n",
    "    for st in stages:\n",
    "        model.train()\n",
    "        num_epochs = st;\n",
    "        for epoch in range(ep,num_epochs):\n",
    "            dataset.update_confidence(model, device)\n",
    "            running_loss = 0.0\n",
    "            for inputs, targets, confidences in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets=targets.to(device) \n",
    "                confidences=confidences.to(device)\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "#                 loss = criterion(outputs, targets)  # Compute loss\n",
    "#                 confidences = torch.softmax(outputs, dim=1)\n",
    "                loss = training_criterion(outputs, targets, confidences)\n",
    "\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Optimize\n",
    "                # scheduler.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "        all_predictions = []\n",
    "        reliable_indice=[]\n",
    "        model.eval()\n",
    "#         pred, reliable_indices = pseudo_labeling(model, dataloaderpred, criterion1, device)\n",
    "#         pseudo_labels1.append(pred)\n",
    "#         confident_labels.append(reliable_indices)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaderpred: \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass to get outputs\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Get the predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                # Calculate per-sample loss\n",
    "                loss_per_sample = criterion1(outputs, labels) \n",
    "\n",
    "            \n",
    "            \n",
    "                # Set a loss threshold (lower loss means more reliable)\n",
    "                reliable_indices = loss_per_sample < 1  # Example threshold for loss\n",
    "\n",
    "                # Extend reliable indices list with boolean values indicating reliability\n",
    "                reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "        pseudo_labels1.append(np.array(all_predictions))\n",
    "        confident_labels.append(np.array(reliable_indice))\n",
    "        ep=st;\n",
    "    result1 = common_elements_same_index(confident_labels)\n",
    "    p.append(result1)\n",
    "    a.append(pseudo_labels1)\n",
    "    print(len(result1))\n",
    "    itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result1, pseudo_labels1)\n",
    "    \n",
    "    if i<2:\n",
    "        augmented_data = []\n",
    "        augmented_labels = []\n",
    "        for idx in result1:\n",
    "            input_data = data[idx]  # Get the data for the current index\n",
    "            input_data = Image.fromarray(input_data)\n",
    "            for _ in range(dmp):  # Perform 5 augmentations\n",
    "                augmented_input = augmentation_transforms(input_data)  # Apply augmentations\n",
    "                augmented_input = (augmented_input.permute(1, 2, 0).numpy()*255).astype(np.uint8)\n",
    "                augmented_data.append(augmented_input)\n",
    "                augmented_labels.append(itr1_pseudo[idx])  # Use the pseudo-label for augmented data\n",
    "\n",
    "        augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "        # Concatenate the original data and augmented data\n",
    "        data = np.concatenate((data, np.array(augmented_data)), axis=0)\n",
    "        itr1_pseudo = np.concatenate((itr1_pseudo, augmented_labels), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset = NDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=512, shuffle=True,num_workers=4)\n",
    "    datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloaderpred = DataLoader(datasetpred, batch_size=512, shuffle=False,num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "    val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    # model.train()\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "    if(i==1):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "net1=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data = data1\n",
    "# # new_labels = new_labels1\n",
    "# val_data = test_images\n",
    "# val_labels = test_labels\n",
    "# val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "# model.eval()\n",
    "# all_predictions = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in val_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "# # model.train()\n",
    "# from sklearn.metrics import f1_score\n",
    "# f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "# print(f'Fold {fold + 1} F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0459\n",
      "Epoch [2/5], Loss: 0.0460\n",
      "Epoch [3/5], Loss: 0.0462\n",
      "Epoch [4/5], Loss: 0.0463\n",
      "Epoch [5/5], Loss: 0.0464\n",
      "Fold 1 F1 Score: 0.0105\n",
      "Epoch [1/4], Loss: 0.0236\n",
      "Epoch [2/4], Loss: 0.4677\n",
      "Epoch [3/4], Loss: 0.1591\n",
      "Epoch [4/4], Loss: 0.1326\n",
      "Fold 1 F1 Score: 0.7131\n",
      "27753\n",
      "Iteration: 0\n",
      "Epoch [1/7], Loss: 0.0514\n",
      "Epoch [2/7], Loss: 0.0602\n",
      "Epoch [3/7], Loss: 0.0605\n",
      "Epoch [4/7], Loss: 0.0564\n",
      "Epoch [5/7], Loss: 0.0533\n",
      "Epoch [6/7], Loss: 0.0485\n",
      "Epoch [7/7], Loss: 0.0444\n",
      "57732\n",
      "Fold 1 F1 Score: 0.7753\n",
      "Iteration: 1\n",
      "Epoch [1/7], Loss: 0.0766\n",
      "Epoch [2/7], Loss: 0.0722\n",
      "Epoch [3/7], Loss: 0.0648\n",
      "Epoch [4/7], Loss: 0.0593\n",
      "Epoch [5/7], Loss: 0.0543\n",
      "Epoch [6/7], Loss: 0.0522\n",
      "Epoch [7/7], Loss: 0.0476\n",
      "111437\n",
      "Fold 1 F1 Score: 0.7718\n",
      "Iteration: 2\n",
      "Epoch [1/7], Loss: 0.0649\n",
      "Epoch [2/7], Loss: 0.0636\n",
      "Epoch [3/7], Loss: 0.0609\n",
      "Epoch [4/7], Loss: 0.0620\n",
      "Epoch [5/7], Loss: 0.0613\n",
      "Epoch [6/7], Loss: 0.0576\n",
      "Epoch [7/7], Loss: 0.0587\n",
      "211679\n",
      "Fold 1 F1 Score: 0.7794\n"
     ]
    }
   ],
   "source": [
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(data1)):\n",
    "from PreResNet import *\n",
    "\n",
    "def create_model():\n",
    "    model = ResNet34(num_classes=100)\n",
    "    model = model.cuda()\n",
    "    return model\n",
    "#     data = data1[train_idx]\n",
    "#     new_labels = new_labels1[train_idx]\n",
    "#     val_data = data1[val_idx]\n",
    "#     val_labels = targets[val_idx]\n",
    "fold = 0\n",
    "\n",
    "data = data1\n",
    "new_labels = new_labels1\n",
    "val_data = val_images\n",
    "val_labels = vallabels\n",
    "\n",
    "#     model = models.resnet50(pretrained=True)  \n",
    "# # model.load_state_dict(torch.load('./2024_08_14-05_29_56_PM/best_cp/modelk.pth',map_location='cuda:0'),strict=False)\n",
    "\n",
    "\n",
    "#     model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# model.to(device)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# criterion1 = nn.CrossEntropyLoss(reduction='none',label_smoothing=0.1)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Model, Loss, and Optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# resnet = models.resnet18(pretrained=False)\n",
    "resnet = create_model()\n",
    "resnet.load_state_dict(torch.load('net2SSLcifar1000.2resnet34.pth'))\n",
    "# resnet.load_state_dict(torch.load('net2cifar1000.5withoutssl.pth'))\n",
    "\n",
    "num_ftrs = resnet.linear.in_features\n",
    "resnet.linear = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "model = nn.Sequential(\n",
    "    resnet,                    # ResNet-50 backbone without classification layer\n",
    "    nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "    nn.ReLU(),                  # ReLU activation function\n",
    "    nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    ")\n",
    "# model=resnet\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# num_ftrs = resnet.fc.in_features\n",
    "# resnet.fc = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "# model = nn.Sequential(\n",
    "#     resnet,                    # ResNet-50 backbone without classification layer\n",
    "#     nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "#     nn.ReLU(),                  # ReLU activation function\n",
    "#     nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.resnet50(pretrained=False)\n",
    "# model.load_state_dict(torch.load('resnet50_weights.pth'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "criterion = NTXentLoss(temperature=0.3)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "dataset = ExampleDataset1(data, new_labels,transform=SimCLRTransform)\n",
    "dataloader = DataLoader(dataset, batch_size=400, shuffle=True,num_workers=4)\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        img_1, img_2 = images  # Two augmented views of the same image\n",
    "\n",
    "        # Move to device\n",
    "        img_1, img_2 = img_1.to(device), img_2.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        z_i = model(img_1)\n",
    "        z_j = model(img_2)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(z_i, z_j)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    \n",
    "    \n",
    "dataset = ExampleDataset1(data, new_labels,transform=SimCLRTransform1)\n",
    "dataloader = DataLoader(dataset, batch_size=200, shuffle=True,num_workers=4)\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        img_1, img_2 = images  # Two augmented views of the same image\n",
    "\n",
    "        # Move to device\n",
    "        img_1, img_2 = img_1.to(device), img_2.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        z_i = model(img_1)\n",
    "        z_j = model(img_2)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(z_i, z_j)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "torch.save(model.state_dict(), 'simclrsslvarybsdividemix1.pth')\n",
    "'''\n",
    "\n",
    "# model.load_state_dict(torch.load('simclrssl1.pth'))\n",
    "# model.to(device)\n",
    "# criterion = NTXentLoss(temperature=0.5)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "# epochs = 20\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, _ in dataloader:\n",
    "#         img_1, img_2 = images  # Two augmented views of the same image\n",
    "\n",
    "#         # Move to device\n",
    "#         img_1, img_2 = img_1.to(device), img_2.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         z_i = model(img_1)\n",
    "#         z_j = model(img_2)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(z_i, z_j)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "# torch.save(model.state_dict(), 'simclrssladamsgd.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# transform = BYOLTransform()\n",
    "# dataset = CIFAR10BYOLDataset(data, new_labels,transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Define BYOL model and optimizer\n",
    "# base_encoder = models.resnet50(pretrained=False)\n",
    "# base_encoder.load_state_dict(torch.load('resnet50_weights.pth'))\n",
    "# model = BYOL(base_encoder).to('cuda')\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Train for a number of epochs\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train_byol(dataloader, model, optimizer)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict(), 'byolssl.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load('simclrsslvarybs.pth'))\n",
    "# model.load_state_dict(torch.load('simclrsslvarybsdividemix1.pth'))\n",
    "# model.load_state_dict(torch.load('byol.pth'))\n",
    "\n",
    "# model.to(device)\n",
    "# model.fc = nn.Sequential(\n",
    "# #     nn.Dropout(0.3),  # Dropout with probability 0.3\n",
    "#     nn.Linear(model.fc.in_features, 10)  # The output layer with 10 classes (for CIFAR-10)\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "#simclr\n",
    "model = nn.Sequential(\n",
    "    model,                    # ResNet-50 backbone without classification layer\n",
    "    nn.ReLU(),                  # ReLU activation function\n",
    "#     nn.Dropout(0.7),\n",
    "    nn.Linear(128, 100)         # Second linear layer projecting to 128 dimensions\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sslafterdividemix\n",
    "# num_ftrs = model.linear.in_features\n",
    "# model.linear = nn.Identity()\n",
    "# model = nn.Sequential(\n",
    "#     model,                    # ResNet-50 backbone without classification layer\n",
    "# #     nn.ReLU(),                  # ReLU activation function\n",
    "# #     nn.Dropout(0.7),\n",
    "#     nn.Linear(num_ftrs, 10)         # Second linear layer projecting to 128 dimensions\n",
    "# )\n",
    "# simclr end\n",
    "\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load('byolssl.pth'))\n",
    "\n",
    "\n",
    "# pretrained_backbone = model.online_encoder\n",
    "\n",
    "# Creating a fine-tuned model for CIFAR-10 classification\n",
    "# model = FineTunedModel(pretrained_backbone).to('cuda')\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     model,                    # ResNet-50 backbone without classification layer\n",
    "#     nn.ReLU(),                  # ReLU activation function\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.Linear(256, 10)         # Second linear layer projecting to 128 dimensions\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# criterion1 = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001,weight_decay=0.03)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.95,weight_decay=5e-4)\n",
    "\n",
    "criterion1 = NCE_MAE_Loss(reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "dataset = NDataset(data, new_labels,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True,num_workers=4)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# initial_lr = 1e-5\n",
    "# final_lr = 1e-4\n",
    "# # Define a linear warm-up schedule\n",
    "# lambda_lr = lambda epoch: 1 + (final_lr - initial_lr) * (epoch / (num_epochs - 1)) / initial_lr\n",
    "# scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    dataset.update_confidence(model, device)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets, confidences in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets=targets.to(device) \n",
    "        confidences=confidences.to(device)\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs = model(inputs)  \n",
    "#         loss = criterion(outputs, targets)\n",
    "#         confidences = torch.softmax(outputs, dim=1)\n",
    "        loss = training_criterion(outputs, targets, confidences)\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step()  \n",
    "#         scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "\n",
    "\n",
    "val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# model.train()\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.95)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "stages = [2,3,4]\n",
    "# stages=[2,4,7,9,11]\n",
    "datasetpred = ExampleDataset(data, new_labels,transform=transform)\n",
    "dataloaderpred = DataLoader(datasetpred, batch_size=512, shuffle=False,num_workers=4)\n",
    "pseudo_labels=[]\n",
    "confident_labels=[]\n",
    "last_epoch=stages[-1]\n",
    "ep=0;\n",
    "for st in stages:\n",
    "    model.train()\n",
    "    num_epochs = st;\n",
    "    for epoch in range(ep,num_epochs):\n",
    "        dataset.update_confidence(model, device)\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets, confidences in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets=targets.to(device) \n",
    "            confidences=confidences.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "#             loss = criterion(outputs, targets)  # Compute loss\n",
    "#             confidences = torch.softmax(outputs, dim=1)\n",
    "            loss = training_criterion(outputs, targets, confidences)\n",
    "        \n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize\n",
    "            # scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "    all_predictions = []\n",
    "    reliable_indice=[]\n",
    "    model.eval()\n",
    "#     pred, reliable_indices = pseudo_labeling(model, dataloaderpred, criterion1, device)\n",
    "#     pseudo_labels.append(pred)\n",
    "#     confident_labels.append(reliable_indices)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaderpred: \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Calculate per-sample loss\n",
    "            loss_per_sample = criterion1(outputs, labels)\n",
    "            \n",
    "\n",
    "\n",
    "            # print(loss_per_sample)\n",
    "            reliable_indices = loss_per_sample < 1  # Example threshold for loss\n",
    "            # print(reliable_indices)\n",
    "            # Extend reliable indices list with boolean values indicating reliability\n",
    "            reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "\n",
    "    pseudo_labels.append(np.array(all_predictions))\n",
    "    confident_labels.append(np.array(reliable_indice))\n",
    "    ep=st;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# model.train()\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "result = common_elements_same_index(confident_labels)\n",
    "print(len(result))\n",
    "itr1_pseudo=new_labels\n",
    "itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result, pseudo_labels)\n",
    "\n",
    "augmented_data = []\n",
    "augmented_labels = []\n",
    "for idx in result:\n",
    "    input_data = data[idx]  # Get the data for the current index\n",
    "    input_data = Image.fromarray(input_data)\n",
    "    for _ in range(dmp):  # Perform 5 augmentations\n",
    "        augmented_input = augmentation_transforms(input_data)  # Apply augmentations\n",
    "        augmented_input = (augmented_input.permute(1, 2, 0).numpy()*255).astype(np.uint8)\n",
    "        augmented_data.append(augmented_input)\n",
    "        augmented_labels.append(itr1_pseudo[idx])  # Use the pseudo-label for augmented data\n",
    "\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Concatenate the original data and augmented data\n",
    "data = np.concatenate((data, np.array(augmented_data)), axis=0)\n",
    "itr1_pseudo = np.concatenate((itr1_pseudo, augmented_labels), axis=0)\n",
    "\n",
    "\n",
    "dataset = NDataset(data, itr1_pseudo,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True,num_workers=4)\n",
    "datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "dataloaderpred = DataLoader(datasetpred, batch_size=512, shuffle=False,num_workers=4)\n",
    "p=[]\n",
    "a=[]\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "augmented_epochs=[1,3,5]\n",
    "for i in range(3):\n",
    "    print(f'Iteration: {i}')\n",
    "    stages = [2,5,7]\n",
    "#     stages = [3,5,6,8,10]\n",
    "    pseudo_labels1=[]\n",
    "    confident_labels=[]\n",
    "    last_epoch=stages[-1]\n",
    "    ep=0;\n",
    "    for st in stages:\n",
    "        model.train()\n",
    "        num_epochs = st;\n",
    "        for epoch in range(ep,num_epochs):\n",
    "            dataset.update_confidence(model, device)\n",
    "            running_loss = 0.0\n",
    "            for inputs, targets, confidences in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets=targets.to(device) \n",
    "                confidences=confidences.to(device)\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "#                 loss = criterion(outputs, targets)  # Compute loss\n",
    "#                 confidences = torch.softmax(outputs, dim=1)\n",
    "                loss = training_criterion(outputs, targets, confidences)\n",
    "\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Optimize\n",
    "                # scheduler.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "        all_predictions = []\n",
    "        reliable_indice=[]\n",
    "        model.eval()\n",
    "#         pred, reliable_indices = pseudo_labeling(model, dataloaderpred, criterion1, device)\n",
    "#         pseudo_labels1.append(pred)\n",
    "#         confident_labels.append(reliable_indices)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaderpred: \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass to get outputs\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Get the predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                # Calculate per-sample loss\n",
    "                loss_per_sample = criterion1(outputs, labels)\n",
    "            \n",
    "\n",
    "                # Set a loss threshold (lower loss means more reliable)\n",
    "                reliable_indices = loss_per_sample < 1  # Example threshold for loss\n",
    "\n",
    "                # Extend reliable indices list with boolean values indicating reliability\n",
    "                reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "        pseudo_labels1.append(np.array(all_predictions))\n",
    "        confident_labels.append(np.array(reliable_indice))\n",
    "        ep=st;\n",
    "    result1 = common_elements_same_index(confident_labels)\n",
    "    p.append(result1)\n",
    "    a.append(pseudo_labels1)\n",
    "    print(len(result1))\n",
    "    itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result1, pseudo_labels1)\n",
    "    \n",
    "    if i <2:\n",
    "        augmented_data = []\n",
    "        augmented_labels = []\n",
    "        for idx in result1:\n",
    "            input_data = data[idx]  # Get the data for the current index\n",
    "            input_data = Image.fromarray(input_data)\n",
    "            for _ in range(dmp):  # Perform 5 augmentations\n",
    "                augmented_input = augmentation_transforms(input_data)  # Apply augmentations\n",
    "                augmented_input = (augmented_input.permute(1, 2, 0).numpy()*255).astype(np.uint8)\n",
    "                augmented_data.append(augmented_input)\n",
    "                augmented_labels.append(itr1_pseudo[idx])  # Use the pseudo-label for augmented data\n",
    "\n",
    "        augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "        # Concatenate the original data and augmented data\n",
    "        data = np.concatenate((data, np.array(augmented_data)), axis=0)\n",
    "        itr1_pseudo = np.concatenate((itr1_pseudo, augmented_labels), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset = NDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=512, shuffle=True,num_workers=4)\n",
    "    datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloaderpred = DataLoader(datasetpred, batch_size=512, shuffle=False,num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "    val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    # model.train()\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "    if(i==1):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "'''    \n",
    "augmented_epochs=[1,2,4]\n",
    "for i in range(0):\n",
    "    print(f'Iteration: {2+i}')\n",
    "    stages = [3,6,10]\n",
    "    pseudo_labels1=[]\n",
    "    confident_labels=[]\n",
    "    last_epoch=stages[-1]\n",
    "    ep=0;\n",
    "    for st in stages:\n",
    "        model.train()\n",
    "        num_epochs = st;\n",
    "        for epoch in range(ep,num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, targets in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets=targets.to(device) \n",
    "                confidences=confidences.to(device)\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "                loss = criterion(outputs, targets)  # Compute loss\n",
    "\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Optimize\n",
    "                # scheduler.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "        all_predictions = []\n",
    "        reliable_indice=[]\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaderpred: \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass to get outputs\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Get the predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                # Calculate per-sample loss\n",
    "                loss_per_sample = criterion1(outputs, labels) \n",
    "\n",
    "                # Set a loss threshold (lower loss means more reliable)\n",
    "                reliable_indices = loss_per_sample < 1  # Example threshold for loss\n",
    "\n",
    "                # Extend reliable indices list with boolean values indicating reliability\n",
    "                reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "        pseudo_labels1.append(np.array(all_predictions))\n",
    "        confident_labels.append(np.array(reliable_indice))\n",
    "        ep=st;\n",
    "    result1 = common_elements_same_index(confident_labels)\n",
    "    p.append(result1)\n",
    "    a.append(pseudo_labels1)\n",
    "    print(len(result1))\n",
    "    itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result1, pseudo_labels1)\n",
    "    \n",
    "#     if i in augmented_epochs:\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "    for idx in result1:\n",
    "        input_data = data[idx]  # Get the data for the current index\n",
    "        input_data = Image.fromarray(input_data)\n",
    "        for _ in range(dmp):  # Perform 5 augmentations\n",
    "            augmented_input = augmentation_transforms(input_data)  # Apply augmentations\n",
    "            augmented_input = (augmented_input.permute(1, 2, 0).numpy()*255).astype(np.uint8)\n",
    "            augmented_data.append(augmented_input)\n",
    "            augmented_labels.append(itr1_pseudo[idx])  # Use the pseudo-label for augmented data\n",
    "\n",
    "    augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "    # Concatenate the original data and augmented data\n",
    "    data = np.concatenate((data, np.array(augmented_data)), axis=0)\n",
    "    itr1_pseudo = np.concatenate((itr1_pseudo, augmented_labels), axis=0)\n",
    "    \n",
    "    \n",
    "    dataset = NDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=512, shuffle=True,num_workers=4)\n",
    "    datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloaderpred = DataLoader(datasetpred, batch_size=512, shuffle=False,num_workers=4)\n",
    "\n",
    "\n",
    "    val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    # model.train()\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "'''\n",
    "net2=model\n",
    "    \n",
    "# for i in range(1):\n",
    "#     print(f'Iteration: {2+i}')\n",
    "#     stages = [2,5]\n",
    "#     pseudo_labels1=[]\n",
    "#     confident_labels=[]\n",
    "#     last_epoch=stages[-1]\n",
    "#     ep=0;\n",
    "#     for st in stages:\n",
    "#         model.train()\n",
    "#         num_epochs = st;\n",
    "#         for epoch in range(ep,num_epochs):\n",
    "#             running_loss = 0.0\n",
    "#             for inputs, targets in dataloader:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 targets=targets.to(device) \n",
    "#                 optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "#                 outputs = model(inputs)  # Forward pass\n",
    "#                 loss = criterion(outputs, targets)  # Compute loss\n",
    "\n",
    "#                 loss.backward()  # Backward pass\n",
    "#                 optimizer.step()  # Optimize\n",
    "#                 # scheduler.step()\n",
    "#                 running_loss += loss.item()\n",
    "\n",
    "#             print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "#         all_predictions = []\n",
    "#         reliable_indice=[]\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in dataloaderpred: \n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 # Forward pass to get outputs\n",
    "#                 outputs = model(inputs)\n",
    "\n",
    "#                 # Get the predictions\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "#                 # Calculate per-sample loss\n",
    "#                 loss_per_sample = criterion1(outputs, labels) \n",
    "\n",
    "#                 # Set a loss threshold (lower loss means more reliable)\n",
    "#                 reliable_indices = loss_per_sample < 1  # Example threshold for loss\n",
    "\n",
    "#                 # Extend reliable indices list with boolean values indicating reliability\n",
    "#                 reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "#         pseudo_labels1.append(np.array(all_predictions))\n",
    "#         confident_labels.append(np.array(reliable_indice))\n",
    "#         ep=st;\n",
    "#     result1 = common_elements_same_index(confident_labels)\n",
    "#     p.append(result1)\n",
    "#     a.append(pseudo_labels1)\n",
    "#     print(len(result1))\n",
    "#     itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result1, pseudo_labels1)\n",
    "#     dataset = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "#     dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "#     datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "#     dataloaderpred = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "#     val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "#     model.eval()\n",
    "#     all_predictions = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             all_predictions.extend(predicted.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#     # model.train()\n",
    "#     from sklearn.metrics import f1_score\n",
    "#     f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "#     print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "\n",
    "# val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "# model.eval()\n",
    "# all_predictions = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in val_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "# # model.train()\n",
    "# from sklearn.metrics import f1_score\n",
    "# f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "# print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1 Score: 0.7949\n"
     ]
    }
   ],
   "source": [
    "val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs1 = net1(inputs)\n",
    "        outputs2 = net2(inputs)\n",
    "        outputs = (outputs1 + outputs2) / 2\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# model.train()\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f'Fold {fold + 1} F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89        57\n",
      "           1       0.86      0.87      0.86        55\n",
      "           2       0.57      0.76      0.65        50\n",
      "           3       0.62      0.50      0.55        52\n",
      "           4       0.53      0.77      0.63        56\n",
      "           5       0.76      0.85      0.81        61\n",
      "           6       0.86      0.86      0.86        64\n",
      "           7       0.79      0.77      0.78        53\n",
      "           8       0.93      0.93      0.93        60\n",
      "           9       0.92      0.92      0.92        59\n",
      "          10       0.77      0.59      0.67        58\n",
      "          11       0.59      0.64      0.62        56\n",
      "          12       0.78      0.84      0.81        58\n",
      "          13       0.88      0.77      0.82        60\n",
      "          14       0.75      0.83      0.79        59\n",
      "          15       0.87      0.87      0.87        69\n",
      "          16       0.69      0.93      0.79        59\n",
      "          17       0.87      0.87      0.87        60\n",
      "          18       0.70      0.73      0.71        62\n",
      "          19       0.90      0.78      0.84        60\n",
      "          20       0.87      0.91      0.89        57\n",
      "          21       0.89      0.92      0.90        62\n",
      "          22       0.81      0.72      0.76        54\n",
      "          23       0.93      0.82      0.87        61\n",
      "          24       0.93      0.87      0.90        60\n",
      "          25       0.80      0.71      0.75        55\n",
      "          26       0.88      0.78      0.83        64\n",
      "          27       0.63      0.78      0.70        59\n",
      "          28       0.87      0.83      0.85        65\n",
      "          29       0.89      0.84      0.86        57\n",
      "          30       0.74      0.70      0.72        64\n",
      "          31       0.84      0.80      0.82        51\n",
      "          32       0.82      0.69      0.75        59\n",
      "          33       0.68      0.69      0.69        65\n",
      "          34       0.82      0.80      0.81        61\n",
      "          35       0.79      0.56      0.65        59\n",
      "          36       0.92      0.80      0.85        59\n",
      "          37       0.85      0.66      0.74        59\n",
      "          38       0.73      0.77      0.75        52\n",
      "          39       0.91      0.92      0.91        63\n",
      "          40       0.80      0.79      0.80        57\n",
      "          41       0.95      0.92      0.93        61\n",
      "          42       0.64      0.84      0.73        56\n",
      "          43       0.90      0.95      0.92        56\n",
      "          44       0.59      0.63      0.61        65\n",
      "          45       0.70      0.69      0.70        58\n",
      "          46       0.60      0.57      0.59        56\n",
      "          47       0.64      0.70      0.67        60\n",
      "          48       0.92      0.94      0.93        71\n",
      "          49       0.81      0.92      0.86        62\n",
      "          50       0.62      0.63      0.63        57\n",
      "          51       0.84      0.90      0.87        52\n",
      "          52       0.58      0.65      0.61        60\n",
      "          53       0.79      0.97      0.87        64\n",
      "          54       0.88      0.85      0.87        61\n",
      "          55       0.57      0.58      0.58        48\n",
      "          56       0.95      0.93      0.94        61\n",
      "          57       0.86      0.79      0.83        63\n",
      "          58       0.89      0.96      0.93        53\n",
      "          59       0.72      0.65      0.68        65\n",
      "          60       0.89      0.88      0.89        58\n",
      "          61       0.77      0.75      0.76        61\n",
      "          62       0.79      0.80      0.80        61\n",
      "          63       0.77      0.71      0.74        58\n",
      "          64       0.80      0.67      0.73        58\n",
      "          65       0.79      0.72      0.75        64\n",
      "          66       0.98      0.87      0.92        62\n",
      "          67       0.63      0.69      0.66        58\n",
      "          68       0.86      0.97      0.91        61\n",
      "          69       0.94      0.84      0.89        70\n",
      "          70       0.85      0.81      0.83        69\n",
      "          71       0.84      0.88      0.86        58\n",
      "          72       0.56      0.60      0.58        63\n",
      "          73       0.61      0.64      0.63        67\n",
      "          74       0.61      0.59      0.60        59\n",
      "          75       0.88      0.92      0.90        65\n",
      "          76       0.93      0.92      0.93        61\n",
      "          77       0.76      0.82      0.79        66\n",
      "          78       0.63      0.75      0.69        64\n",
      "          79       0.75      0.90      0.82        62\n",
      "          80       0.83      0.73      0.78        62\n",
      "          81       0.83      0.88      0.85        66\n",
      "          82       0.95      0.94      0.94        64\n",
      "          83       0.77      0.75      0.76        65\n",
      "          84       0.81      0.81      0.81        64\n",
      "          85       0.98      0.89      0.94        65\n",
      "          86       0.91      0.83      0.87        64\n",
      "          87       0.78      0.90      0.84        62\n",
      "          88       0.89      0.86      0.88        59\n",
      "          89       0.85      0.94      0.89        65\n",
      "          90       0.83      0.84      0.83        63\n",
      "          91       0.94      0.87      0.90        52\n",
      "          92       0.86      0.71      0.78        59\n",
      "          93       0.74      0.78      0.76        55\n",
      "          94       0.96      0.98      0.97        50\n",
      "          95       0.80      0.79      0.79        61\n",
      "          96       0.81      0.65      0.72        65\n",
      "          97       0.96      0.75      0.84        57\n",
      "          98       0.68      0.62      0.65        52\n",
      "          99       0.83      0.77      0.80        65\n",
      "\n",
      "    accuracy                           0.80      6000\n",
      "   macro avg       0.80      0.79      0.79      6000\n",
      "weighted avg       0.80      0.80      0.80      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Generate report\n",
    "print(classification_report(all_labels, all_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def common_elements_same_index(arrays):\n",
    "#     if not arrays:\n",
    "#         return []\n",
    "\n",
    "#     # Number of arrays\n",
    "#     num_arrays = len(arrays)\n",
    "    \n",
    "#     # Length of the first array\n",
    "#     num_elements = len(arrays[0])\n",
    "\n",
    "#     # Store indices where all arrays have the same element\n",
    "#     common_indices = []\n",
    "    \n",
    "#     # Iterate over each index\n",
    "#     for i in range(num_elements):\n",
    "#         # Get the element at index i in the first array\n",
    "#         element = arrays[0][i]\n",
    "        \n",
    "#         # Check if all arrays have the same element at index i\n",
    "#         if all(array[i] == element for array in arrays):\n",
    "#             common_indices.append(i)\n",
    "    \n",
    "#     return common_indices\n",
    "\n",
    "\n",
    "# result1 = common_elements_same_index(pseudo_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2083934,
     "status": "ok",
     "timestamp": 1723895235061,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "rKvtkfHa-oho",
    "outputId": "68e43e1e-db07-4ddb-da59-37ae73dee515"
   },
   "outputs": [],
   "source": [
    "# # num_epochs = 5\n",
    "# running_loss=50;\n",
    "# epoch=0;\n",
    "# while True:\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, targets in dataloader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         targets=targets.to(device) \n",
    "#         optimizer.zero_grad()  \n",
    "\n",
    "#         outputs = model(inputs) \n",
    "#         loss = criterion(outputs, targets)  \n",
    "\n",
    "#         loss.backward() \n",
    "#         optimizer.step() \n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "# #   print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "#     print(f'Epoch [{epoch + 1}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "#     if running_loss / len(dataloader)<0.6:\n",
    "#         break;\n",
    "#     epoch=epoch+1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "UP2edhhu-zPp"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'resnet50_weights_cifar10_wbyol_loss.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35116,
     "status": "ok",
     "timestamp": 1723906830244,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "z6cZJNqBLU4h",
    "outputId": "1b13cb2e-baef-4fc4-acb4-fd25807b9254"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"0.0.layer1.0.conv3.weight\", \"0.0.layer1.0.bn3.weight\", \"0.0.layer1.0.bn3.bias\", \"0.0.layer1.0.bn3.running_mean\", \"0.0.layer1.0.bn3.running_var\", \"0.0.layer1.0.downsample.0.weight\", \"0.0.layer1.0.downsample.1.weight\", \"0.0.layer1.0.downsample.1.bias\", \"0.0.layer1.0.downsample.1.running_mean\", \"0.0.layer1.0.downsample.1.running_var\", \"0.0.layer1.1.conv3.weight\", \"0.0.layer1.1.bn3.weight\", \"0.0.layer1.1.bn3.bias\", \"0.0.layer1.1.bn3.running_mean\", \"0.0.layer1.1.bn3.running_var\", \"0.0.layer1.2.conv3.weight\", \"0.0.layer1.2.bn3.weight\", \"0.0.layer1.2.bn3.bias\", \"0.0.layer1.2.bn3.running_mean\", \"0.0.layer1.2.bn3.running_var\", \"0.0.layer2.0.conv3.weight\", \"0.0.layer2.0.bn3.weight\", \"0.0.layer2.0.bn3.bias\", \"0.0.layer2.0.bn3.running_mean\", \"0.0.layer2.0.bn3.running_var\", \"0.0.layer2.0.downsample.0.weight\", \"0.0.layer2.0.downsample.1.weight\", \"0.0.layer2.0.downsample.1.bias\", \"0.0.layer2.0.downsample.1.running_mean\", \"0.0.layer2.0.downsample.1.running_var\", \"0.0.layer2.1.conv3.weight\", \"0.0.layer2.1.bn3.weight\", \"0.0.layer2.1.bn3.bias\", \"0.0.layer2.1.bn3.running_mean\", \"0.0.layer2.1.bn3.running_var\", \"0.0.layer2.2.conv3.weight\", \"0.0.layer2.2.bn3.weight\", \"0.0.layer2.2.bn3.bias\", \"0.0.layer2.2.bn3.running_mean\", \"0.0.layer2.2.bn3.running_var\", \"0.0.layer2.3.conv3.weight\", \"0.0.layer2.3.bn3.weight\", \"0.0.layer2.3.bn3.bias\", \"0.0.layer2.3.bn3.running_mean\", \"0.0.layer2.3.bn3.running_var\", \"0.0.layer3.0.conv3.weight\", \"0.0.layer3.0.bn3.weight\", \"0.0.layer3.0.bn3.bias\", \"0.0.layer3.0.bn3.running_mean\", \"0.0.layer3.0.bn3.running_var\", \"0.0.layer3.0.downsample.0.weight\", \"0.0.layer3.0.downsample.1.weight\", \"0.0.layer3.0.downsample.1.bias\", \"0.0.layer3.0.downsample.1.running_mean\", \"0.0.layer3.0.downsample.1.running_var\", \"0.0.layer3.1.conv3.weight\", \"0.0.layer3.1.bn3.weight\", \"0.0.layer3.1.bn3.bias\", \"0.0.layer3.1.bn3.running_mean\", \"0.0.layer3.1.bn3.running_var\", \"0.0.layer3.2.conv3.weight\", \"0.0.layer3.2.bn3.weight\", \"0.0.layer3.2.bn3.bias\", \"0.0.layer3.2.bn3.running_mean\", \"0.0.layer3.2.bn3.running_var\", \"0.0.layer3.3.conv3.weight\", \"0.0.layer3.3.bn3.weight\", \"0.0.layer3.3.bn3.bias\", \"0.0.layer3.3.bn3.running_mean\", \"0.0.layer3.3.bn3.running_var\", \"0.0.layer3.4.conv3.weight\", \"0.0.layer3.4.bn3.weight\", \"0.0.layer3.4.bn3.bias\", \"0.0.layer3.4.bn3.running_mean\", \"0.0.layer3.4.bn3.running_var\", \"0.0.layer3.5.conv3.weight\", \"0.0.layer3.5.bn3.weight\", \"0.0.layer3.5.bn3.bias\", \"0.0.layer3.5.bn3.running_mean\", \"0.0.layer3.5.bn3.running_var\", \"0.0.layer4.0.conv3.weight\", \"0.0.layer4.0.bn3.weight\", \"0.0.layer4.0.bn3.bias\", \"0.0.layer4.0.bn3.running_mean\", \"0.0.layer4.0.bn3.running_var\", \"0.0.layer4.0.downsample.0.weight\", \"0.0.layer4.0.downsample.1.weight\", \"0.0.layer4.0.downsample.1.bias\", \"0.0.layer4.0.downsample.1.running_mean\", \"0.0.layer4.0.downsample.1.running_var\", \"0.0.layer4.1.conv3.weight\", \"0.0.layer4.1.bn3.weight\", \"0.0.layer4.1.bn3.bias\", \"0.0.layer4.1.bn3.running_mean\", \"0.0.layer4.1.bn3.running_var\", \"0.0.layer4.2.conv3.weight\", \"0.0.layer4.2.bn3.weight\", \"0.0.layer4.2.bn3.bias\", \"0.0.layer4.2.bn3.running_mean\", \"0.0.layer4.2.bn3.running_var\", \"3.weight\", \"3.bias\". \n\tUnexpected key(s) in state_dict: \"0.0.layer2.0.shortcut.0.weight\", \"0.0.layer2.0.shortcut.1.weight\", \"0.0.layer2.0.shortcut.1.bias\", \"0.0.layer2.0.shortcut.1.running_mean\", \"0.0.layer2.0.shortcut.1.running_var\", \"0.0.layer2.0.shortcut.1.num_batches_tracked\", \"0.0.layer3.0.shortcut.0.weight\", \"0.0.layer3.0.shortcut.1.weight\", \"0.0.layer3.0.shortcut.1.bias\", \"0.0.layer3.0.shortcut.1.running_mean\", \"0.0.layer3.0.shortcut.1.running_var\", \"0.0.layer3.0.shortcut.1.num_batches_tracked\", \"0.0.layer4.0.shortcut.0.weight\", \"0.0.layer4.0.shortcut.1.weight\", \"0.0.layer4.0.shortcut.1.bias\", \"0.0.layer4.0.shortcut.1.running_mean\", \"0.0.layer4.0.shortcut.1.running_var\", \"0.0.layer4.0.shortcut.1.num_batches_tracked\", \"2.weight\", \"2.bias\". \n\tsize mismatch for 0.0.conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\n\tsize mismatch for 0.0.layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for 0.0.layer1.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for 0.0.layer1.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for 0.0.layer2.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for 0.0.layer2.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for 0.0.layer2.2.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for 0.0.layer2.3.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for 0.0.layer3.0.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1]).\n\tsize mismatch for 0.0.layer3.1.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer3.2.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer3.3.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer3.4.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer3.5.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer4.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer4.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for 0.0.layer4.2.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for 0.1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     25\u001b[0m     model,                    \u001b[38;5;66;03m# ResNet-50 backbone without classification layer\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),                  \u001b[38;5;66;03m# ReLU activation function\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.3\u001b[39m),\n\u001b[1;32m     28\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m10\u001b[39m)         \u001b[38;5;66;03m# Second linear layer projecting to 128 dimensions\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# model = models.resnet50(pretrained=True)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# model.fc = nn.Linear(model.fc.in_features, 10)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet50_weights_cifar10_wbyol_loss.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# transform = transforms.Compose([\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# #     transforms.ToPILImage(),  \u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#     transforms.ToTensor(),\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#      transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# ])\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1605\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1600\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1601\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1602\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1606\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"0.0.layer1.0.conv3.weight\", \"0.0.layer1.0.bn3.weight\", \"0.0.layer1.0.bn3.bias\", \"0.0.layer1.0.bn3.running_mean\", \"0.0.layer1.0.bn3.running_var\", \"0.0.layer1.0.downsample.0.weight\", \"0.0.layer1.0.downsample.1.weight\", \"0.0.layer1.0.downsample.1.bias\", \"0.0.layer1.0.downsample.1.running_mean\", \"0.0.layer1.0.downsample.1.running_var\", \"0.0.layer1.1.conv3.weight\", \"0.0.layer1.1.bn3.weight\", \"0.0.layer1.1.bn3.bias\", \"0.0.layer1.1.bn3.running_mean\", \"0.0.layer1.1.bn3.running_var\", \"0.0.layer1.2.conv3.weight\", \"0.0.layer1.2.bn3.weight\", \"0.0.layer1.2.bn3.bias\", \"0.0.layer1.2.bn3.running_mean\", \"0.0.layer1.2.bn3.running_var\", \"0.0.layer2.0.conv3.weight\", \"0.0.layer2.0.bn3.weight\", \"0.0.layer2.0.bn3.bias\", \"0.0.layer2.0.bn3.running_mean\", \"0.0.layer2.0.bn3.running_var\", \"0.0.layer2.0.downsample.0.weight\", \"0.0.layer2.0.downsample.1.weight\", \"0.0.layer2.0.downsample.1.bias\", \"0.0.layer2.0.downsample.1.running_mean\", \"0.0.layer2.0.downsample.1.running_var\", \"0.0.layer2.1.conv3.weight\", \"0.0.layer2.1.bn3.weight\", \"0.0.layer2.1.bn3.bias\", \"0.0.layer2.1.bn3.running_mean\", \"0.0.layer2.1.bn3.running_var\", \"0.0.layer2.2.conv3.weight\", \"0.0.layer2.2.bn3.weight\", \"0.0.layer2.2.bn3.bias\", \"0.0.layer2.2.bn3.running_mean\", \"0.0.layer2.2.bn3.running_var\", \"0.0.layer2.3.conv3.weight\", \"0.0.layer2.3.bn3.weight\", \"0.0.layer2.3.bn3.bias\", \"0.0.layer2.3.bn3.running_mean\", \"0.0.layer2.3.bn3.running_var\", \"0.0.layer3.0.conv3.weight\", \"0.0.layer3.0.bn3.weight\", \"0.0.layer3.0.bn3.bias\", \"0.0.layer3.0.bn3.running_mean\", \"0.0.layer3.0.bn3.running_var\", \"0.0.layer3.0.downsample.0.weight\", \"0.0.layer3.0.downsample.1.weight\", \"0.0.layer3.0.downsample.1.bias\", \"0.0.layer3.0.downsample.1.running_mean\", \"0.0.layer3.0.downsample.1.running_var\", \"0.0.layer3.1.conv3.weight\", \"0.0.layer3.1.bn3.weight\", \"0.0.layer3.1.bn3.bias\", \"0.0.layer3.1.bn3.running_mean\", \"0.0.layer3.1.bn3.running_var\", \"0.0.layer3.2.conv3.weight\", \"0.0.layer3.2.bn3.weight\", \"0.0.layer3.2.bn3.bias\", \"0.0.layer3.2.bn3.running_mean\", \"0.0.layer3.2.bn3.running_var\", \"0.0.layer3.3.conv3.weight\", \"0.0.layer3.3.bn3.weight\", \"0.0.layer3.3.bn3.bias\", \"0.0.layer3.3.bn3.running_mean\", \"0.0.layer3.3.bn3.running_var\", \"0.0.layer3.4.conv3.weight\", \"0.0.layer3.4.bn3.weight\", \"0.0.layer3.4.bn3.bias\", \"0.0.layer3.4.bn3.running_mean\", \"0.0.layer3.4.bn3.running_var\", \"0.0.layer3.5.conv3.weight\", \"0.0.layer3.5.bn3.weight\", \"0.0.layer3.5.bn3.bias\", \"0.0.layer3.5.bn3.running_mean\", \"0.0.layer3.5.bn3.running_var\", \"0.0.layer4.0.conv3.weight\", \"0.0.layer4.0.bn3.weight\", \"0.0.layer4.0.bn3.bias\", \"0.0.layer4.0.bn3.running_mean\", \"0.0.layer4.0.bn3.running_var\", \"0.0.layer4.0.downsample.0.weight\", \"0.0.layer4.0.downsample.1.weight\", \"0.0.layer4.0.downsample.1.bias\", \"0.0.layer4.0.downsample.1.running_mean\", \"0.0.layer4.0.downsample.1.running_var\", \"0.0.layer4.1.conv3.weight\", \"0.0.layer4.1.bn3.weight\", \"0.0.layer4.1.bn3.bias\", \"0.0.layer4.1.bn3.running_mean\", \"0.0.layer4.1.bn3.running_var\", \"0.0.layer4.2.conv3.weight\", \"0.0.layer4.2.bn3.weight\", \"0.0.layer4.2.bn3.bias\", \"0.0.layer4.2.bn3.running_mean\", \"0.0.layer4.2.bn3.running_var\", \"3.weight\", \"3.bias\". \n\tUnexpected key(s) in state_dict: \"0.0.layer2.0.shortcut.0.weight\", \"0.0.layer2.0.shortcut.1.weight\", \"0.0.layer2.0.shortcut.1.bias\", \"0.0.layer2.0.shortcut.1.running_mean\", \"0.0.layer2.0.shortcut.1.running_var\", \"0.0.layer2.0.shortcut.1.num_batches_tracked\", \"0.0.layer3.0.shortcut.0.weight\", \"0.0.layer3.0.shortcut.1.weight\", \"0.0.layer3.0.shortcut.1.bias\", \"0.0.layer3.0.shortcut.1.running_mean\", \"0.0.layer3.0.shortcut.1.running_var\", \"0.0.layer3.0.shortcut.1.num_batches_tracked\", \"0.0.layer4.0.shortcut.0.weight\", \"0.0.layer4.0.shortcut.1.weight\", \"0.0.layer4.0.shortcut.1.bias\", \"0.0.layer4.0.shortcut.1.running_mean\", \"0.0.layer4.0.shortcut.1.running_var\", \"0.0.layer4.0.shortcut.1.num_batches_tracked\", \"2.weight\", \"2.bias\". \n\tsize mismatch for 0.0.conv1.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\n\tsize mismatch for 0.0.layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for 0.0.layer1.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for 0.0.layer1.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for 0.0.layer2.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for 0.0.layer2.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for 0.0.layer2.2.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for 0.0.layer2.3.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for 0.0.layer3.0.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1]).\n\tsize mismatch for 0.0.layer3.1.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer3.2.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer3.3.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer3.4.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer3.5.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer4.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for 0.0.layer4.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for 0.0.layer4.2.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for 0.1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "resnet = models.resnet50(pretrained=False)\n",
    "# resnet.load_state_dict(torch.load('resnet50_weights.pth'))\n",
    "\n",
    "# Remove the classification layer (fully connected layer)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "\n",
    "# Define the projection head (2-layer MLP) and include it with the ResNet model\n",
    "model = nn.Sequential(\n",
    "    resnet,                    # ResNet-50 backbone without classification layer\n",
    "    nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "    nn.ReLU(),                  # ReLU activation function\n",
    "    nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    ")\n",
    "\n",
    "model = nn.Sequential(\n",
    "    model,                    # ResNet-50 backbone without classification layer\n",
    "    nn.ReLU(),                  # ReLU activation function\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(128, 10)         # Second linear layer projecting to 128 dimensions\n",
    ")\n",
    "\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model.load_state_dict(torch.load('resnet50_weights_cifar10_wbyol_loss.pth'))\n",
    "model.eval()\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "# #     transforms.ToPILImage(),  \n",
    "#     transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "# ])\n",
    "\n",
    "test_images = np.load('data/cifar100/test_images.npy')\n",
    "test_labels = np.load('data/cifar100/test_labels.npy')\n",
    "\n",
    "# test_dataset = ExampleDataset(test_images, test_labels,transform)\n",
    "test_images=test_images.reshape(-1, 3, 32, 32)\n",
    "data=test_images\n",
    "data=data.reshape(-1, 3, 32, 32)\n",
    "data=data.transpose((0, 2, 3, 1))\n",
    "testl = torch.from_numpy(test_labels)\n",
    "test_dataset=ExampleDataset(data,testl,transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels=labels.to(device) \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(np.array(all_predictions[:1000])-np.array(testl[:1000]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1723911251111,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "nrBmCCe2ts62",
    "outputId": "07c5682d-dd4e-46d9-9ea1-891aa3d34e07"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(np.array(all_labels), np.array(all_predictions))\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchmetrics.classification import MulticlassF1Score\n",
    "# f1_score = MulticlassF1Score(num_classes=10, average='macro')\n",
    "\n",
    "# all_predictions = torch.tensor(all_predictions)\n",
    "# all_labels = torch.tensor(all_labels)\n",
    "\n",
    "# f1_score.update(all_predictions, all_labels)\n",
    "# f1 = f1_score.compute()\n",
    "\n",
    "# print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f1)\n",
    "# print(f'Fold {fold + 1} F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torch\n",
    "\n",
    "# Assuming all_predictions and all_labels are defined as lists or arrays\n",
    "all_predictions = torch.tensor(all_predictions)\n",
    "all_labels = torch.tensor(all_labels)\n",
    "\n",
    "# Initialize accuracy metric for multiclass classification\n",
    "accuracy_metric = MulticlassAccuracy(num_classes=10)\n",
    "\n",
    "# Update metric with predictions and labels\n",
    "accuracy_metric.update(all_predictions, all_labels)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = accuracy_metric.compute()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0QmFwdP4pTN"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from torchvision import transforms, datasets\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import torchvision.models as models\n",
    "# import torch.nn as nn\n",
    "\n",
    "# model = models.resnet50(pretrained=False)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# model.load_state_dict(torch.load('resnet50_weights_cifar10.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# # transform = transforms.Compose([\n",
    "# # #     transforms.ToPILImage(),  \n",
    "#  #   transforms.ToTensor(),\n",
    "#  #   transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "# # ])\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data, transform=None):\n",
    "#         self.data = data\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         x = self.data[index]\n",
    "#         if self.transform:\n",
    "#             x = self.transform(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# original_images = np.load('data/cifar100/train_images.npy')\n",
    "# data=original_images\n",
    "# data=data.reshape(-1, 3, 32, 32)\n",
    "# data=data.transpose((0, 2, 3, 1))\n",
    "\n",
    "# dataset = CustomDataset(images, transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# all_predictions = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs in dataloader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# pseudo_labels = np.array(all_predictions)\n",
    "\n",
    "# np.save('pseudo_labels.npy', pseudo_labels)\n",
    "\n",
    "# print(f'Pseudo labels have been saved to pseudo_labels.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_images = np.load('data/cifar100/train_images.npy')\n",
    "# pseudo_labels = np.load('pseudo_labels.npy')\n",
    "# data=original_images\n",
    "# data=data.reshape(-1, 3, 32, 32)\n",
    "# data=data.transpose((0, 2, 3, 1))\n",
    "# targets = torch.from_numpy(pseudo_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ExampleDataset(data, targets,transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet50(pretrained=False)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 5\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, targets in dataloader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         targets=targets.to(device) \n",
    "#         optimizer.zero_grad() \n",
    "\n",
    "#         outputs = model(inputs)  \n",
    "#         loss = criterion(outputs, targets)  \n",
    "\n",
    "#         loss.backward() \n",
    "#         optimizer.step()  \n",
    "#         # scheduler.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'resnet50_weights_cifar10.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.datasets import MNIST\n",
    "# from torchvision import models\n",
    "# import torch.nn as nn\n",
    "\n",
    "# model = models.resnet50(pretrained=False)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# model.load_state_dict(torch.load('resnet50_weights_cifar10.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# # transform = transforms.Compose([\n",
    "# # #     transforms.ToPILImage(),  \n",
    "# #     transforms.ToTensor(),\n",
    "# #      transforms.Normalize((0.5071,0.4867,0.4408),(0.2675,0.2565,0.2761)),\n",
    "# # ])\n",
    "\n",
    "# test_images = np.load('data/cifar100/test_images.npy')\n",
    "# test_labels = np.load('data/cifar100/test_labels.npy')\n",
    "\n",
    "# test_images=test_images.reshape(-1, 3, 32, 32)\n",
    "# data=test_images\n",
    "# data=data.reshape(-1, 3, 32, 32)\n",
    "# data=data.transpose((0, 2, 3, 1))\n",
    "# testl = torch.from_numpy(test_labels)\n",
    "# test_dataset=ExampleDataset(data,testl,transform=transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# all_predictions = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in test_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels=labels.to(device) \n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# print(np.array(all_predictions[:1000])-np.array(testl[:1000]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(np.array(all_labels), np.array(all_predictions))\n",
    "\n",
    "# print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchmetrics.classification import MulticlassF1Score\n",
    "# f1_score = MulticlassF1Score(num_classes=10, average='macro')\n",
    "\n",
    "# all_predictions = torch.tensor(all_predictions)\n",
    "# all_labels = torch.tensor(all_labels)\n",
    "\n",
    "# f1_score.update(all_predictions, all_labels)\n",
    "# f1 = f1_score.compute()\n",
    "\n",
    "# print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOk4guyS0I0FTG3Yna/p2Aa",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
