{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1723906795135,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "AUjR-lho0VYd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "from PIL import Image\n",
    "\n",
    "import tools\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"../HAM/HAM10000_metadata.csv\")\n",
    "df=df[[\"image_id\",\"dx\"]]\n",
    "mapping = {\n",
    "    \"nv\": 0,\n",
    "    \"mel\": 1,\n",
    "    \"bkl\":2,\n",
    "    \"bcc\": 3,\n",
    "    \"vasc\": 4,\n",
    "    \"df\": 5,\n",
    "    \"akiec\": 6\n",
    "}\n",
    "\n",
    "# Map values directly\n",
    "df[\"Hlabels\"] = df[\"dx\"].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3926 images.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm  # Progress bar for large datasets\n",
    "\n",
    "# Folder containing the .tif images\n",
    "folder_path = '../HAM/images/'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "# List to store images\n",
    "Himages = []\n",
    "Hlabels=[]\n",
    "no0=0;\n",
    "# Iterate through the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.jpg'):\n",
    "        if(int(df[df[\"image_id\"]==str(filename[0:-4])][\"Hlabels\"]) in [0,1,2,3]):\n",
    "            if(int(df[df[\"image_id\"]==str(filename[0:-4])][\"Hlabels\"])==0):\n",
    "                no0=no0+1\n",
    "                if(no0>1200):\n",
    "                    continue\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "    #         img = transform(img)\n",
    "            img = np.array(img).flatten()\n",
    "            Himages.append(np.array(img))\n",
    "            filename=filename[0:-4]\n",
    "            Hlabels.append(int(df[df[\"image_id\"]==str(filename)][\"Hlabels\"]))\n",
    "                \n",
    "Himages = np.array(Himages) \n",
    "Hlabels=np.array(Hlabels)\n",
    "# Check the number of images loaded\n",
    "print(f\"Loaded {len(Himages)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hlabels=torch.from_numpy(Hlabels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%)\n",
    "trainimg, test_images, trla, test_labels = train_test_split(Himages, Hlabels, test_size=0.12, random_state=42)\n",
    "Himages, val_images, labels, vallabels = train_test_split(trainimg, trla, test_size=0.14, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=torch.from_numpy(Himages).float().to(device)\n",
    "targets=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19775,
     "status": "ok",
     "timestamp": 1723892057717,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "cpgi5MZm0YVn",
    "outputId": "05881838-f253-44b1-c104-c6b86598bf9a"
   },
   "outputs": [],
   "source": [
    "# original_images = np.load('data/cifar10/train_images.npy')\n",
    "# original_labels = np.load('data/cifar10/train_labels.npy') \n",
    "# data1 = torch.from_numpy(original_images).float().to(device)\n",
    "# targets = torch.from_numpy(original_labels).to(device)\n",
    "dataset = zip(data1, targets)\n",
    "data1=Himages\n",
    "# data1=data1.reshape(-1, 3, 32, 32)\n",
    "# data1=data1.transpose((0, 2, 3, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19775,
     "status": "ok",
     "timestamp": 1723892057717,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "cpgi5MZm0YVn",
    "outputId": "05881838-f253-44b1-c104-c6b86598bf9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building dataset...\n"
     ]
    }
   ],
   "source": [
    "noise_rate = 0.2\n",
    "bsize=16\n",
    "lr=0.0001\n",
    "num_classes = 4\n",
    "feature_size = 224*224*3\n",
    "norm_std = 0.1\n",
    "seed = 1\n",
    "new_labels1 = tools.get_instance_noisy_label(noise_rate, dataset, targets, num_classes, feature_size, norm_std, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pickle\n",
    "\n",
    "# def save_cifar_batch(images, labels, batch_label, filename):\n",
    "#     \"\"\"\n",
    "#     Save images and labels in the CIFAR-like format using pickle.\n",
    "\n",
    "#     Args:\n",
    "#     - images: NumPy array of shape (N, 3, 32, 32).\n",
    "#     - labels: List of integer labels.\n",
    "#     - batch_label: String label for the batch.\n",
    "#     - filename: Path where the pickle file will be saved.\n",
    "#     \"\"\"\n",
    "#     # Flatten the images from (N, 3, 32, 32) to (N, 3072)\n",
    "#     flattened_images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "#     # Create a dictionary similar to the CIFAR format\n",
    "#     batch_dict = {\n",
    "#         'data': flattened_images,\n",
    "#         'labels': labels,\n",
    "#         'batch_label': batch_label\n",
    "#     }\n",
    "\n",
    "#     # Save the dictionary as a pickle file\n",
    "#     with open(filename, 'wb') as f:\n",
    "#         pickle.dump(batch_dict, f)\n",
    "\n",
    "# def split_and_save_batches(images, labels, num_batches=5, prefix=\"data_batch_\"):\n",
    "#     \"\"\"\n",
    "#     Split images and labels into several batches and save each as a pickle file.\n",
    "\n",
    "#     Args:\n",
    "#     - images: NumPy array of shape (N, 3, 32, 32).\n",
    "#     - labels: List of integer labels.\n",
    "#     - num_batches: Number of batches to split the data into.\n",
    "#     - prefix: Prefix for the batch file names.\n",
    "#     \"\"\"\n",
    "#     N = images.shape[0]\n",
    "#     batch_size = N // num_batches\n",
    "\n",
    "#     for i in range(num_batches):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = (i + 1) * batch_size if i != num_batches - 1 else N  # Include remaining data in the last batch\n",
    "#         batch_images = images[start_idx:end_idx]\n",
    "#         batch_labels = labels[start_idx:end_idx]\n",
    "#         batch_label = f\"training batch {i+1} of {num_batches}\"\n",
    "\n",
    "#         # Save the batch to a pickle file\n",
    "#         filename = \"./HAM0.2/\"+f\"{prefix}{i+1}.pkl\"\n",
    "#         save_cifar_batch(batch_images, batch_labels, batch_label, filename)\n",
    "#         print(f\"Saved {filename} with {len(batch_labels)} images.\")\n",
    "\n",
    "\n",
    "\n",
    "# # Split and save into 5 batches\n",
    "# split_and_save_batches(data1, new_labels1, num_batches=5, prefix=\"data_batch_\")\n",
    "# split_and_save_batches(test_images, test_labels, num_batches=1, prefix=\"test_batch_\")\n",
    "# # test_images = np.load('data/cifar10/test_images.npy')\n",
    "# # test_labels = np.load('data/cifar10/test_labels.npy')\n",
    "# # testl = torch.from_numpy(test_labels)\n",
    "\n",
    "\n",
    "# # split_and_save_batches(test_images, testl, num_batches=1, prefix=\"test_batch_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data1.reshape(-1, 3,224, 224)\n",
    "\n",
    "# data1=data1.reshape(-1, 3, 600, 400)\n",
    "data1=data1.transpose((0, 2, 3, 1))\n",
    "\n",
    "test_images=test_images.reshape(-1, 3,224, 224)\n",
    "test_images=test_images.transpose((0, 2, 3, 1))\n",
    "\n",
    "val_images=val_images.reshape(-1, 3,224, 224)\n",
    "val_images=val_images.transpose((0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2970, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r checkpoint\n",
    "# !mkdir checkpoint\n",
    "# !rm -rf ./cifar10/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1723906795135,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "cuzwQBWQ0xT7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tBPggGb5BNyh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=torch.load('./2024_08_14-05_29_56_PM/best_cp/modelk.pth',map_location='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # Load the pretrained ResNet-50 model\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# # Freeze the initial half of the layers\n",
    "# total_layers = len(list(model.children()))  # Total number of layers\n",
    "# freeze_layers = total_layers // 2  # Freeze the first half\n",
    "\n",
    "# for i, child in enumerate(model.children()):\n",
    "#     if i < freeze_layers:\n",
    "#         for param in child.parameters():\n",
    "#             param.requires_grad = False\n",
    "#     else:\n",
    "#         for param in child.parameters():\n",
    "#             param.requires_grad = True\n",
    "# model.to(device)\n",
    "# # Verify which layers are frozen\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f'{name}: {\"Frozen\" if not param.requires_grad else \"Trainable\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision.models as models\n",
    "# import torch.nn.init as init\n",
    "\n",
    "# # Load the pretrained ResNet-50 model\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# # Define a function to apply Xavier initialization\n",
    "# def initialize_weights_xavier(model):\n",
    "#     for m in model.modules():\n",
    "#         if isinstance(m, (nn.Conv2d, nn.Linear)):  # Apply Xavier to Conv2d and Linear layers\n",
    "#             init.xavier_uniform_(m.weight)  # Xavier uniform distribution\n",
    "#             if m.bias is not None:  # If bias exists, initialize it to 0\n",
    "#                 init.constant_(m.bias, 0)\n",
    "\n",
    "# # Apply Xavier initialization to the model\n",
    "# initialize_weights_xavier(model)\n",
    "# model.to(device)\n",
    "# # Check the model to confirm initialization\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tBPggGb5BNyh"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision.models as models\n",
    "\n",
    "\n",
    "# model = models.resnet50(pretrained=True)  \n",
    "\n",
    "\n",
    "# # model.load_state_dict(torch.load('./2024_08_14-05_29_56_PM/best_cp/modelk.pth',map_location='cuda:0'),strict=False)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# model.to(device)\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# # optimizer.load_state_dict(torch.load('./2024_08_14-05_29_56_PM/best_cp/optimizer.pth',map_location='cuda:0'))\n",
    "\n",
    "# # # Initialize Predictor (if it's also a ResNet-18 or another model)\n",
    "# # predictor = models.resnet18(pretrained=False)\n",
    "# # predictor.load_state_dict(torch.load('./2024_08_14-05_29_56_PM/last_cp/predictor.pth'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Assuming images are in shape (N, 224, 224, 3) and dtype uint8\n",
    "# # Normalize to [0,1] before computing mean/std\n",
    "# images = data1.astype(np.float32) / 255.0\n",
    "\n",
    "# # Compute mean and std along (0,1,2) axes (over all pixels)\n",
    "# mean = images.mean(axis=(0, 1, 2))\n",
    "# std = images.std(axis=(0, 1, 2))\n",
    "\n",
    "# print(f\"Mean: {mean.tolist()}\")\n",
    "# print(f\"Std: {std.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SFeBO7gpBSAm"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Adjusts crop scaling for large images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.6429, 0.5844, 0.6333), (0.1659, 0.1909, 0.1631)), \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1723906795135,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "ULX5Vrbar6C2"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "class ExampleDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = Image.fromarray(x)\n",
    "        y = self.labels[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)  \n",
    "            \n",
    "        return x, y\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = Image.fromarray(x)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_elements_same_index(arrays):\n",
    "    if not arrays:\n",
    "        return []\n",
    "\n",
    "    # Number of arrays\n",
    "    num_arrays = len(arrays)\n",
    "    \n",
    "    # Length of the first array\n",
    "    num_elements = len(arrays[0])\n",
    "\n",
    "    # Store indices where all arrays have the same element\n",
    "    common_indices = []\n",
    "    \n",
    "    # Iterate over each index\n",
    "    for i in range(num_elements):\n",
    "        if all(array[i] == 1 for array in arrays):\n",
    "            common_indices.append(i)\n",
    "    \n",
    "    return common_indices\n",
    "\n",
    "\n",
    "# result = common_elements_same_index(confident_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itr1_pseudo=new_labels\n",
    "def replace_elements_at_indices(another_array, common_indices, arrays):\n",
    "    k=len(arrays)\n",
    "    for i in common_indices:\n",
    "        another_array[i] = arrays[k-1][i]\n",
    "    \n",
    "    return another_array\n",
    "\n",
    "# itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result, pseudo_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Unfreeze all layers for further fine-tuning\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True  # Unfreeze all parameters\n",
    "\n",
    "# # Redefine the optimizer to include all parameters now that all layers are unfrozen\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)  # Use a smaller learning rate for fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 4\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Data Augmentation (BYOL Transform for CIFAR-10)\n",
    "class BYOLTransform:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=32),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.6429, 0.5844, 0.6333), (0.1659, 0.1909, 0.1631)),  \n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.transform(x), self.transform(x)\n",
    "\n",
    "# MLP for projection and prediction head\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=4096, out_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# BYOL model with backbone, projector, and predictor\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(self, base_encoder, out_dim=256, momentum=0.996):\n",
    "        super(BYOL, self).__init__()\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Online network\n",
    "        self.online_encoder = base_encoder\n",
    "        num_ftrs = self.online_encoder.fc.in_features\n",
    "        self.online_encoder.fc = MLP(num_ftrs, out_dim=out_dim)\n",
    "        \n",
    "        # Target network (initialized with online network weights)\n",
    "        self.target_encoder = base_encoder\n",
    "        self.target_encoder.load_state_dict(self.online_encoder.state_dict())\n",
    "        self.target_encoder.fc = MLP(num_ftrs, out_dim=out_dim)\n",
    "        \n",
    "        # Predictor\n",
    "        self.predictor = MLP(out_dim, out_dim=out_dim)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_target_encoder(self):\n",
    "        # Momentum update for the target encoder\n",
    "        for online_params, target_params in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):\n",
    "            target_params.data = self.momentum * target_params.data + (1.0 - self.momentum) * online_params.data\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Online network forward pass\n",
    "        online_proj_1 = self.online_encoder(x1)\n",
    "        online_proj_2 = self.online_encoder(x2)\n",
    "        \n",
    "        # Predict from online projection\n",
    "        pred_1 = self.predictor(online_proj_1)\n",
    "        pred_2 = self.predictor(online_proj_2)\n",
    "        \n",
    "        # Target network forward pass (no gradient computation)\n",
    "        with torch.no_grad():\n",
    "            self._momentum_update_target_encoder()  # update the target encoder\n",
    "            target_proj_1 = self.target_encoder(x1)\n",
    "            target_proj_2 = self.target_encoder(x2)\n",
    "        \n",
    "        return pred_1, pred_2, target_proj_1, target_proj_2\n",
    "\n",
    "# Loss function (mean squared error between predictions and targets)\n",
    "def byol_loss_fn(pred, target):\n",
    "    pred = F.normalize(pred, dim=-1, p=2)\n",
    "    target = F.normalize(target, dim=-1, p=2)\n",
    "    return 2 - 2 * (pred * target).sum(dim=-1).mean()\n",
    "\n",
    "# CIFAR-10 Dataset class\n",
    "class CIFAR10BYOLDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = Image.fromarray(x)\n",
    "        y = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x_i, x_j = self.transform(x)\n",
    "        else:\n",
    "            x_i, x_j = x, x\n",
    "\n",
    "        return (x_i, x_j), y\n",
    "\n",
    "# Training function for BYOL with CIFAR-10\n",
    "def train_byol(data_loader, model, optimizer, device='cuda'):\n",
    "    model.train()\n",
    "    for (x_i, x_j), _ in data_loader:\n",
    "        x_i = x_i.to(device)\n",
    "        x_j = x_j.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_1, pred_2, target_1, target_2 = model(x_i, x_j)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_1 = byol_loss_fn(pred_1, target_2)\n",
    "        loss_2 = byol_loss_fn(pred_2, target_1)\n",
    "        loss = (loss_1 + loss_2) / 2\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, pretrained_backbone, out_dim=10):\n",
    "        super(FineTunedModel, self).__init__()\n",
    "        self.backbone = pretrained_backbone  # Pretrained SSL backbone (ResNet)\n",
    "        \n",
    "        # Freeze the backbone parameters (optional, can be unfrozen later for fine-tuning)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # New classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(),               # ReLU activation\n",
    "            nn.Dropout(0.3),          # Dropout with 30% probability\n",
    "            nn.Linear(256, out_dim)   # Classification layer with output size 10 (for CIFAR-10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)       # Pass input through the pretrained backbone\n",
    "        x = self.classifier(x)     # Pass through the classifier head\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Augmentation (SimCLR Transform)\n",
    "# class SimCLRTransform:\n",
    "#     def __init__(self, size=224):\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.RandomResizedCrop(size=size),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "#             transforms.RandomGrayscale(p=0.2),\n",
    "#             transforms.GaussianBlur(kernel_size=9),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.467, 0.459, 0.519), (0.207, 0.233, 0.261)),\n",
    "#         ])\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         # Apply the transformation twice to create two views\n",
    "#         return self.transform(x), self.transform(x)\n",
    "\n",
    "\n",
    "SimCLRTransform=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=112),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=9),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.6429, 0.5844, 0.6333), (0.1659, 0.1909, 0.1631)),  \n",
    "        ])\n",
    "\n",
    "SimCLRTransform1=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=9),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.6429, 0.5844, 0.6333), (0.1659, 0.1909, 0.1631)),   \n",
    "        ])\n",
    "\n",
    "# ResNet Backbone with Projection Head\n",
    "class ResNetSimCLR(nn.Module):\n",
    "    def __init__(self, out_dim=128):\n",
    "        super(ResNetSimCLR, self).__init__()\n",
    "        # Load a pre-trained ResNet-50 model\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Remove the classification layer\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # Projection head: 2-layer MLP\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.projector(x)\n",
    "        return x\n",
    "\n",
    "# NT-Xent Loss (Contrastive Loss)\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        # Normalize the projections\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "        # Concatenate both projections\n",
    "        batch_size = z_i.shape[0]\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "\n",
    "        # Compute cosine similarity matrix\n",
    "        similarity_matrix = torch.matmul(representations, representations.T) / self.temperature\n",
    "\n",
    "        # Create labels for positive pairs\n",
    "        labels = torch.cat([torch.arange(batch_size) for _ in range(2)], dim=0).to(z_i.device)\n",
    "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "\n",
    "        # Mask out the diagonal (self-similarity)\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(z_i.device)\n",
    "        labels = labels[~mask].view(labels.shape[0], -1)\n",
    "        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
    "\n",
    "        # Compute loss\n",
    "        positives = similarity_matrix[labels.bool()].view(batch_size, -1)\n",
    "        negatives = similarity_matrix[~labels.bool()].view(batch_size, -1)\n",
    "\n",
    "        logits = torch.cat([positives, negatives], dim=1)\n",
    "        targets = torch.zeros(logits.shape[0], dtype=torch.long).to(z_i.device)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "    \n",
    "class ExampleDataset1(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the original image\n",
    "        x = self.data[index]\n",
    "        x = Image.fromarray(x)\n",
    "        y = self.labels[index]\n",
    "\n",
    "        # Apply the SimCLR transformations (two different augmented views of the same image)\n",
    "        if self.transform:\n",
    "            x_i = self.transform(x)  # First view\n",
    "            x_j = self.transform(x)  # Second view\n",
    "        else:\n",
    "            x_i, x_j = x, x  # In case no transformation is applied\n",
    "\n",
    "        return (x_i, x_j), y  # Return both views and the label (y) if applicable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_images = np.load('data/cifar10/test_images.npy')\n",
    "# test_labels = np.load('data/cifar10/test_labels.npy')\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # test_dataset = ExampleDataset(test_images, test_labels,transform)\n",
    "# test_images=test_images.reshape(-1, 3, 32, 32)\n",
    "# datatest=test_images\n",
    "# datatest=datatest.reshape(-1, 3, 32, 32)\n",
    "# datatest=datatest.transpose((0, 2, 3, 1))\n",
    "# testl = torch.from_numpy(test_labels)\n",
    "\n",
    "# val_images, test_images, vallabels, test_labels = train_test_split(\n",
    "#     datatest, testl, test_size=0.4, random_state=42\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation_transforms = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(size=32, scale=(0.8, 1.0)),  # Resize and crop\n",
    "#     transforms.RandomHorizontalFlip(),  # Horizontal flip\n",
    "#     transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),  # Color jitter\n",
    "#     transforms.RandomGrayscale(p=0.2),  # Convert to grayscale randomly\n",
    "#     transforms.GaussianBlur(kernel_size=3),  # Apply Gaussian blur\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "augmentation_transforms = transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Adjusted for 224x224\n",
    "        transforms.RandomHorizontalFlip(),                    # Horizontal flip\n",
    "#         transforms.ColorJitter(brightness=0.2, contrast=0.2), # Color jitter\n",
    "#         transforms.RandomGrayscale(p=0.1)                     # Grayscale randomly\n",
    "        \n",
    "        transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for fold, (train_idx, val_idx) in enumerate(kf.split(data1)):\n",
    "# from PreResNet import *\n",
    "\n",
    "# def create_model():\n",
    "#     model = ResNet18(num_classes=4)\n",
    "#     model = model.cuda()\n",
    "#     return model\n",
    "# #     data = data1[train_idx]\n",
    "# #     new_labels = new_labels1[train_idx]\n",
    "# #     val_data = data1[val_idx]\n",
    "# #     val_labels = targets[val_idx]\n",
    "# fold = 0\n",
    "\n",
    "# data = data1\n",
    "# new_labels = new_labels1\n",
    "# val_data = val_images\n",
    "# val_labels = vallabels\n",
    "\n",
    "# #     model = models.resnet50(pretrained=True)  \n",
    "# # # model.load_state_dict(torch.load('./2024_08_14-05_29_56_PM/best_cp/modelk.pth',map_location='cuda:0'),strict=False)\n",
    "\n",
    "\n",
    "# #     model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# # model.to(device)\n",
    "# # model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# # model.to(device)\n",
    "\n",
    "\n",
    "# # criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# # criterion1 = nn.CrossEntropyLoss(reduction='none',label_smoothing=0.1)\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # Model, Loss, and Optimizer\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# # resnet = models.resnet18(pretrained=False)\n",
    "# resnet = create_model()\n",
    "# # resnet.load_state_dict(torch.load('net1withoutssl0.5.pth'))\n",
    "# # num_ftrs = resnet.linear.in_features\n",
    "# # resnet.linear = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "# # model = nn.Sequential(\n",
    "# #     resnet,                    # ResNet-50 backbone without classification layer\n",
    "# #     nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "# #     nn.ReLU(),                  # ReLU activation function\n",
    "# #     nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    "# # )\n",
    "# model=resnet\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # num_ftrs = resnet.fc.in_features\n",
    "# # resnet.fc = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "# # model = nn.Sequential(\n",
    "# #     resnet,                    # ResNet-50 backbone without classification layer\n",
    "# #     nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "# #     nn.ReLU(),                  # ReLU activation function\n",
    "# #     nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    "# # )\n",
    "# # model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# # model = models.resnet50(pretrained=False)\n",
    "# # model.load_state_dict(torch.load('resnet50_weights.pth'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# criterion = NTXentLoss(temperature=0.3)\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "# dataset = ExampleDataset1(data, new_labels,transform=SimCLRTransform)\n",
    "# dataloader = DataLoader(dataset, batch_size=64, shuffle=True,num_workers=4)\n",
    "# epochs = 15\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, _ in dataloader:\n",
    "#         img_1, img_2 = images  # Two augmented views of the same image\n",
    "\n",
    "#         # Move to device\n",
    "#         img_1, img_2 = img_1.to(device), img_2.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         z_i = model(img_1)\n",
    "#         z_j = model(img_2)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(z_i, z_j)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    \n",
    "    \n",
    "# dataset = ExampleDataset1(data, new_labels,transform=SimCLRTransform1)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True,num_workers=4)\n",
    "# epochs = 15\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, _ in dataloader:\n",
    "#         img_1, img_2 = images  # Two augmented views of the same image\n",
    "\n",
    "#         # Move to device\n",
    "#         img_1, img_2 = img_1.to(device), img_2.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         z_i = model(img_1)\n",
    "#         z_j = model(img_2)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(z_i, z_j)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# torch.save(model.state_dict(), 'HAMsslonly.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class RobustLoss(nn.Module):\n",
    "#     def __init__(self, alpha=0.25, gamma=2.0, label_smoothing=0.1, reduction='mean'):\n",
    "#         super(RobustLoss, self).__init__()\n",
    "#         self.alpha = alpha  # Weighting factor for balancing positive/negative class\n",
    "#         self.gamma = gamma  # Focusing parameter\n",
    "#         self.label_smoothing = label_smoothing\n",
    "#         self.reduction = reduction\n",
    "\n",
    "#     def forward(self, outputs, targets):\n",
    "#         # Apply label smoothing\n",
    "#         num_classes = outputs.size(1)\n",
    "#         one_hot_targets = torch.zeros_like(outputs).scatter_(1, targets.unsqueeze(1), 1)\n",
    "#         smooth_target = (1 - self.label_smoothing) * one_hot_targets + self.label_smoothing / num_classes\n",
    "\n",
    "#         # Compute cross-entropy loss\n",
    "#         log_p = F.log_softmax(outputs, dim=1)\n",
    "#         loss = -smooth_target * log_p\n",
    "\n",
    "#         # Focal Loss\n",
    "#         p_t = torch.exp(-loss)  # Probability of the true class\n",
    "#         focal_loss = self.alpha * (1 - p_t) ** self.gamma * loss\n",
    "\n",
    "#         # Reduce the loss\n",
    "#         if self.reduction == 'mean':\n",
    "#             return focal_loss.mean()\n",
    "#         elif self.reduction == 'sum':\n",
    "#             return focal_loss.sum()\n",
    "#         else:\n",
    "#             return focal_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# class NDataset(Dataset):\n",
    "#     def __init__(self, images, labels, transform=None, num_classes=10):\n",
    "#         \"\"\"\n",
    "#         A dataset class that maintains label confidence scores for noisy datasets.\n",
    "\n",
    "#         Args:\n",
    "#             images (list or numpy.ndarray): The dataset of images.\n",
    "#             labels (list or numpy.ndarray): The noisy labels corresponding to the images.\n",
    "#             transform (callable, optional): A function/transform to apply to the images.\n",
    "#             num_classes (int, optional): Number of classes in the dataset (default is 10).\n",
    "#         \"\"\"\n",
    "#         self.images = images\n",
    "#         self.labels = np.array(labels)\n",
    "#         self.transform = transform\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#         # Initialize label confidence scores uniformly\n",
    "#         self.label_confidence = torch.ones(len(self.labels), self.num_classes) / self.num_classes\n",
    "\n",
    "#     def update_confidence(self, model, device, m=0, batch_size=bsize, num_workers=4):\n",
    "#         \"\"\"\n",
    "#         Updates label confidence scores based on the given mathematical expression.\n",
    "\n",
    "#         Args:\n",
    "#             model (torch.nn.Module): The trained model.\n",
    "#             device (torch.device): The device to run the model on (e.g., 'cpu' or 'cuda').\n",
    "#             criterion: Loss function to compute L(f(x; θ), y).\n",
    "#             m (float): A variable that affects the size of |Dce|.\n",
    "#             batch_size (int): The number of images per batch.\n",
    "#             num_workers (int): Number of subprocesses for data loading (default: 4).\n",
    "#         \"\"\"\n",
    "#         model = model.to(device)\n",
    "#         model.eval()\n",
    "\n",
    "#         dataset = ExampleDataset(self.images, self.labels,self.transform)\n",
    "#         data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "#         criterion = nn.CrossEntropyLoss(label_smoothing=0.1, reduction='none')\n",
    "        \n",
    "\n",
    "#         total_loss = 0.0\n",
    "#         num_samples = 0\n",
    "#         losses = []\n",
    "\n",
    "#         # Compute losses for the entire dataset\n",
    "#         with torch.no_grad():\n",
    "#             for batch_images, batch_labels in data_loader:\n",
    "#                 batch_images = batch_images.to(device)\n",
    "#                 batch_labels = batch_labels.to(device)\n",
    "\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(batch_images)\n",
    "# #                     batch_labels = torch.tensor(self.labels[num_samples:num_samples + batch_images.size(0)], device=device)\n",
    "\n",
    "#                 # Compute loss\n",
    "#                 loss = criterion(outputs, batch_labels)\n",
    "#                 losses.append(loss)\n",
    "#                 total_loss += loss.sum().item()\n",
    "#                 num_samples += batch_images.size(0)\n",
    "\n",
    "#         # Compute mean loss µ\n",
    "#         mu = total_loss / num_samples\n",
    "\n",
    "#         # Update confidence scores using the formula\n",
    "#         with torch.no_grad():\n",
    "#             for batch_idx, (batch_images,batch_labels) in enumerate(data_loader):\n",
    "#                 batch_images = batch_images.to(device)\n",
    "#                 batch_labels = batch_labels.to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(batch_images)\n",
    "# #                     batch_labels = torch.tensor(self.labels[batch_idx * batch_size : batch_idx * batch_size + batch_images.size(0)], device=device)\n",
    "\n",
    "#                 # Compute loss\n",
    "#                 loss = criterion(outputs, batch_labels)\n",
    "\n",
    "#                 # Compute h(f(x;θ)y) using the given formula\n",
    "#                 h_values = torch.sigmoid(0.5 * (-loss + mu + m))\n",
    "\n",
    "#                 # Update confidence scores\n",
    "#                 start_idx = batch_idx * batch_size\n",
    "#                 end_idx = start_idx + batch_images.size(0)\n",
    "#                 self.label_confidence[start_idx:end_idx] = h_values.cpu().unsqueeze(1).repeat(1, self.num_classes)\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Returns the image, label, and confidence scores at the given index.\n",
    "\n",
    "#         Args:\n",
    "#             idx (int): The index of the data point.\n",
    "\n",
    "#         Returns:\n",
    "#             tuple: (image, label, confidence_scores)\n",
    "#         \"\"\"\n",
    "#         image = Image.fromarray(self.images[idx])\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         label = self.labels[idx]\n",
    "#         confidence = self.label_confidence[idx]\n",
    "#         return image, label, confidence\n",
    "\n",
    "\n",
    "\n",
    "# # Step 4: Define Training Loss Function with Confidence Incorporation\n",
    "# class ConfidenceWeightedLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConfidenceWeightedLoss, self).__init__()\n",
    "#         self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1, reduction='none')\n",
    "        \n",
    "\n",
    "#     def forward(self, outputs, targets, confidences):\n",
    "#         losses = self.criterion(outputs, targets)\n",
    "#         log_probs = F.log_softmax(outputs, dim=1)\n",
    "#         Deno = torch.sum(log_probs, dim=1)\n",
    "#         losses1=-losses/(Deno+1e-8)\n",
    "#         weighted_loss = (losses1+losses * confidences[range(len(targets)), targets]).mean()\n",
    "#         return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class NDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None, num_classes=4):\n",
    "        \"\"\"\n",
    "        A dataset class that maintains label confidence scores for noisy datasets.\n",
    "\n",
    "        Args:\n",
    "            images (list or numpy.ndarray): The dataset of images.\n",
    "            labels (list or numpy.ndarray): The noisy labels corresponding to the images.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "            num_classes (int, optional): Number of classes in the dataset (default is 10).\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = np.array(labels)\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Initialize label confidence scores uniformly\n",
    "        self.label_confidence = torch.ones(len(self.labels), self.num_classes) / self.num_classes\n",
    "\n",
    "    def update_confidence(self, model, device, m=0, batch_size=bsize, num_workers=4):\n",
    "        \"\"\"\n",
    "        Updates label confidence scores based on the given mathematical expression.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The trained model.\n",
    "            device (torch.device): The device to run the model on (e.g., 'cpu' or 'cuda').\n",
    "            criterion: Loss function to compute L(f(x; θ), y).\n",
    "            m (float): A variable that affects the size of |Dce|.\n",
    "            batch_size (int): The number of images per batch.\n",
    "            num_workers (int): Number of subprocesses for data loading (default: 4).\n",
    "        \"\"\"\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        dataset = ExampleDataset(self.images, self.labels,self.transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1, reduction='none')\n",
    "        \n",
    "\n",
    "#         total_loss = 0.0\n",
    "#         num_samples = 0\n",
    "#         losses = []\n",
    "\n",
    "#         # Compute losses for the entire dataset\n",
    "#         with torch.no_grad():\n",
    "#             for batch_images, batch_labels in data_loader:\n",
    "#                 batch_images = batch_images.to(device)\n",
    "#                 batch_labels = batch_labels.to(device)\n",
    "\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(batch_images)\n",
    "# #                     batch_labels = torch.tensor(self.labels[num_samples:num_samples + batch_images.size(0)], device=device)\n",
    "\n",
    "#                 # Compute loss\n",
    "#                 loss = criterion(outputs, batch_labels)\n",
    "#                 losses.append(loss)\n",
    "#                 total_loss += loss.sum().item()\n",
    "#                 num_samples += batch_images.size(0)\n",
    "\n",
    "#         # Compute mean loss µ\n",
    "#         mu = total_loss / num_samples\n",
    "\n",
    "        # Update confidence scores using the formula\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (batch_images,batch_labels) in enumerate(data_loader):\n",
    "                batch_images = batch_images.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_images)\n",
    "#                     batch_labels = torch.tensor(self.labels[batch_idx * batch_size : batch_idx * batch_size + batch_images.size(0)], device=device)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                # Compute loss\n",
    "#                 loss = criterion(outputs, batch_labels)\n",
    "\n",
    "#                 # Compute h(f(x;θ)y) using the given formula\n",
    "#                 h_values = torch.sigmoid(0.5 * (-loss + mu + m))\n",
    "\n",
    "                # Update confidence scores\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_images.size(0)\n",
    "                self.label_confidence[start_idx:end_idx] = probs.cpu()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the image, label, and confidence scores at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the data point.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, label, confidence_scores)\n",
    "        \"\"\"\n",
    "        image = Image.fromarray(self.images[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        confidence = self.label_confidence[idx]\n",
    "        return image, label, confidence\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Define Training Loss Function with Confidence Incorporation\n",
    "class ConfidenceWeightedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConfidenceWeightedLoss, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "    def forward(self, outputs, targets, confidences):\n",
    "        losses = self.criterion(outputs, targets)\n",
    "#         log_probs = F.log_softmax(outputs, dim=1)\n",
    "#         Deno = torch.sum(log_probs, dim=1)\n",
    "#         losses1=-losses/(Deno+1e-8)\n",
    "#         weighted_loss = (losses * confidences[range(len(targets)), targets]).mean()\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def dynamic_loss_thresholding(losses, labels, percentile=30):\n",
    "    \"\"\"Computes class-wise dynamic loss threshold based on the given percentile while preserving order.\"\"\"\n",
    "    thresholds = {}\n",
    "    reliable_indices = torch.zeros_like(losses, dtype=torch.bool)\n",
    "    \n",
    "    unique_classes = torch.unique(labels)\n",
    "    for cls in unique_classes:\n",
    "        class_mask = labels == cls\n",
    "        class_losses = losses[class_mask]\n",
    "        if len(class_losses) > 0:\n",
    "            threshold = np.percentile(class_losses.cpu().numpy(), percentile)\n",
    "            thresholds[cls.item()] = threshold\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        cls = labels[i].item()\n",
    "        if cls in thresholds and losses[i] < thresholds[cls]:\n",
    "            reliable_indices[i] = True\n",
    "    \n",
    "    return reliable_indices\n",
    "\n",
    "def pseudo_labeling(model, dataloader, criterion1, device, percentile=30):\n",
    "    \"\"\"Generates pseudo-labels and selects reliable samples using dynamic loss thresholding while preserving order.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_losses = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            loss_per_sample = criterion1(outputs, labels)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_losses.extend(loss_per_sample.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_losses = torch.tensor(all_losses, device=device)\n",
    "    all_labels = torch.tensor(all_labels, device=device)\n",
    "    \n",
    "    reliable_indices = dynamic_loss_thresholding(all_losses, all_labels, percentile)\n",
    "    reliable_indices_np = reliable_indices.cpu().numpy().astype(int)  # Convert to binary 1/0\n",
    "    \n",
    "    return all_predictions, reliable_indices_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weighting factor for class imbalance\n",
    "        self.gamma = gamma  # Focusing parameter\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")  # Compute cross-entropy loss\n",
    "        pt = torch.exp(-ce_loss)  # Probabilities of correct class\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss  # Apply focal weight\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss  # No reduction\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1772\n",
      "Epoch [2/5], Loss: 0.1707\n",
      "Epoch [3/5], Loss: 0.1690\n",
      "Epoch [4/5], Loss: 0.1644\n",
      "Epoch [5/5], Loss: 0.1664\n",
      "Fold 1 F1 Score: 0.4294\n",
      "Epoch [1/4], Loss: 0.1659\n",
      "Epoch [2/4], Loss: 0.1643\n",
      "Epoch [3/4], Loss: 0.1815\n",
      "Epoch [4/4], Loss: 0.1718\n",
      "Fold 1 F1 Score: 0.2862\n",
      "1992\n",
      "Iteration: 0\n",
      "Epoch [1/7], Loss: 0.1167\n",
      "Epoch [2/7], Loss: 0.0984\n",
      "Epoch [3/7], Loss: 0.0955\n",
      "Epoch [4/7], Loss: 0.0943\n",
      "Epoch [5/7], Loss: 0.0932\n",
      "Epoch [6/7], Loss: 0.0932\n",
      "Epoch [7/7], Loss: 0.0929\n",
      "6079\n",
      "Fold 1 F1 Score: 0.2457\n",
      "Iteration: 1\n",
      "Epoch [1/7], Loss: 0.0524\n",
      "Epoch [2/7], Loss: 0.0508\n",
      "Epoch [3/7], Loss: 0.0484\n",
      "Epoch [4/7], Loss: 0.0478\n",
      "Epoch [5/7], Loss: 0.0473\n",
      "Epoch [6/7], Loss: 0.0461\n",
      "Epoch [7/7], Loss: 0.0457\n",
      "18111\n",
      "Fold 1 F1 Score: 0.2372\n",
      "Iteration: 2\n",
      "Epoch [1/7], Loss: 0.0290\n",
      "Epoch [2/7], Loss: 0.0278\n",
      "Epoch [3/7], Loss: 0.0277\n",
      "Epoch [4/7], Loss: 0.0275\n",
      "Epoch [5/7], Loss: 0.0273\n",
      "Epoch [6/7], Loss: 0.0273\n",
      "Epoch [7/7], Loss: 0.0272\n",
      "54216\n",
      "Fold 1 F1 Score: 0.2400\n"
     ]
    }
   ],
   "source": [
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(data1)):\n",
    "from PreResNet import *\n",
    "dmp=2\n",
    "\n",
    "def create_model():\n",
    "    model = ResNet18(num_classes=4)\n",
    "    model = model.cuda()\n",
    "    return model\n",
    "\n",
    "fold = 0\n",
    "\n",
    "data = data1\n",
    "new_labels = new_labels1\n",
    "val_data = val_images\n",
    "val_labels = vallabels\n",
    "\n",
    "\n",
    "\n",
    "# # Model, Loss, and Optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# resnet = models.resnet18(pretrained=False)\n",
    "resnet = create_model()\n",
    "# resnet.load_state_dict(torch.load('net1SSLcifar100.3.pth'))\n",
    "# resnet.load_state_dict(torch.load('net1SSLcifar100.3.pth'))\n",
    "# resnet.load_state_dict(torch.load('net1sslafterdividemix.pth'))\n",
    "# resnet.load_state_dict(torch.load('sslthendividemix.pth'))\n",
    "resnet.load_state_dict(torch.load('net1HAM0.2.pth'))\n",
    "# resnet.load_state_dict(torch.load('chestxsslonlyfewbatches.pth'))\n",
    "\n",
    "num_ftrs = resnet.linear.in_features\n",
    "resnet.linear = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "model = nn.Sequential(\n",
    "    resnet,                    # ResNet-50 backbone without classification layer\n",
    "    nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "    nn.ReLU(),                  # ReLU activation function\n",
    "    nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#simclr\n",
    "model = nn.Sequential(\n",
    "    model,                    # ResNet-50 backbone without classification layer\n",
    "    nn.ReLU(),                  # ReLU activation function\n",
    "#     nn.Dropout(0.7),\n",
    "    nn.Linear(128, 4)         # Second linear layer projecting to 128 dimensions\n",
    ")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "criterion1 = nn.CrossEntropyLoss(reduction='none')\n",
    "training_criterion = ConfidenceWeightedLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.95,weight_decay=5e-4)\n",
    "\n",
    "\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "criterion1 = FocalLoss(alpha=0.25, gamma=2.0,reduction='none')\n",
    "\n",
    "\n",
    "dataset = NDataset(data, new_labels,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True,num_workers=4)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     dataset.update_confidence(model, device)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets, confidences in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets=targets.to(device)\n",
    "        confidences=confidences.to(device)\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs = model(inputs)  \n",
    "        loss = criterion(outputs, targets)\n",
    "# #         confidences = torch.softmax(outputs, dim=1)\n",
    "#         loss = training_criterion(outputs, targets, confidences)\n",
    "\n",
    "        loss.backward() \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()  \n",
    "        # scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "\n",
    "\n",
    "val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# model.train()\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.95)\n",
    "\n",
    "\n",
    "stages = [2,3,4]\n",
    "# stages=[2,4,7,9,11]\n",
    "datasetpred = ExampleDataset(data, new_labels,transform=transform)\n",
    "dataloaderpred = DataLoader(datasetpred, batch_size=bsize, shuffle=False,num_workers=4)\n",
    "pseudo_labels=[]\n",
    "confident_labels=[]\n",
    "last_epoch=stages[-1]\n",
    "ep=0;\n",
    "model.train()\n",
    "for st in stages:\n",
    "    num_epochs = st;\n",
    "    for epoch in range(ep,num_epochs):\n",
    "#         dataset.update_confidence(model, device)\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets, confidences in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets=targets.to(device) \n",
    "            confidences=confidences.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute loss\n",
    "# #             confidences = torch.softmax(outputs, dim=1)\n",
    "#             loss = training_criterion(outputs, targets, confidences)\n",
    "\n",
    "            loss.backward()  # Backward pass\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()  # Optimize\n",
    "            # scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "    all_predictions = []\n",
    "    reliable_indice=[]\n",
    "    model.eval()\n",
    "#     pred, reliable_indices = pseudo_labeling(model, dataloaderpred, criterion1, device)\n",
    "#     pseudo_labels.append(pred)\n",
    "#     confident_labels.append(reliable_indices)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaderpred: \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Calculate per-sample loss\n",
    "            loss_per_sample = criterion1(outputs, labels) \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # print(loss_per_sample)\n",
    "            reliable_indices = loss_per_sample < 0.25  # Example threshold for loss\n",
    "            # print(reliable_indices)\n",
    "            # Extend reliable indices list with boolean values indicating reliability\n",
    "            reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "\n",
    "    pseudo_labels.append(np.array(all_predictions))\n",
    "    confident_labels.append(np.array(reliable_indice))\n",
    "    ep=st;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# model.train()\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "result = common_elements_same_index(confident_labels)\n",
    "print(len(result))\n",
    "itr1_pseudo=new_labels\n",
    "itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result, pseudo_labels)\n",
    "\n",
    "augmented_data = []\n",
    "augmented_labels = []\n",
    "for idx in result:\n",
    "    input_data = data[idx]  # Get the data for the current index\n",
    "    input_data = Image.fromarray(input_data)\n",
    "    for _ in range(dmp):  # Perform 5 augmentations\n",
    "        augmented_input = augmentation_transforms(input_data)  # Apply augmentations\n",
    "        augmented_input = (augmented_input.permute(1, 2, 0).numpy()*255).astype(np.uint8)\n",
    "        augmented_data.append(augmented_input)\n",
    "        augmented_labels.append(itr1_pseudo[idx])  # Use the pseudo-label for augmented data\n",
    "\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Concatenate the original data and augmented data\n",
    "data = np.concatenate((data, np.array(augmented_data)), axis=0)\n",
    "itr1_pseudo = np.concatenate((itr1_pseudo, augmented_labels), axis=0)\n",
    "\n",
    "\n",
    "dataset = NDataset(data, itr1_pseudo,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True,num_workers=4)\n",
    "datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "dataloaderpred = DataLoader(datasetpred, batch_size=bsize, shuffle=False,num_workers=4)\n",
    "p=[]\n",
    "a=[]\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "augmented_epochs=[1,3,5]\n",
    "for i in range(3):\n",
    "    print(f'Iteration: {i}')\n",
    "    stages = [2,5,7]\n",
    "#     stages = [3,5,6,8,10]\n",
    "    pseudo_labels1=[]\n",
    "    confident_labels=[]\n",
    "    last_epoch=stages[-1]\n",
    "    ep=0;\n",
    "    for st in stages:\n",
    "        model.train()\n",
    "        num_epochs = st;\n",
    "        for epoch in range(ep,num_epochs):\n",
    "#             dataset.update_confidence(model, device)\n",
    "            running_loss = 0.0\n",
    "            for inputs, targets, confidences in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets=targets.to(device) \n",
    "                confidences=confidences.to(device)\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "                loss = criterion(outputs, targets)  # Compute loss\n",
    "# #                 confidences = torch.softmax(outputs, dim=1)\n",
    "#                 loss = training_criterion(outputs, targets, confidences)\n",
    "\n",
    "                loss.backward()  # Backward pass\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()  # Optimize\n",
    "                # scheduler.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "        all_predictions = []\n",
    "        reliable_indice=[]\n",
    "        model.eval()\n",
    "#         pred, reliable_indices = pseudo_labeling(model, dataloaderpred, criterion1, device)\n",
    "#         pseudo_labels1.append(pred)\n",
    "#         confident_labels.append(reliable_indices)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaderpred: \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass to get outputs\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Get the predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                # Calculate per-sample loss\n",
    "                loss_per_sample = criterion1(outputs, labels) \n",
    "\n",
    "            \n",
    "            \n",
    "                # Set a loss threshold (lower loss means more reliable)\n",
    "                reliable_indices = loss_per_sample < 0.25  # Example threshold for loss\n",
    "\n",
    "                # Extend reliable indices list with boolean values indicating reliability\n",
    "                reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "        pseudo_labels1.append(np.array(all_predictions))\n",
    "        confident_labels.append(np.array(reliable_indice))\n",
    "        ep=st;\n",
    "    result1 = common_elements_same_index(confident_labels)\n",
    "    p.append(result1)\n",
    "    a.append(pseudo_labels1)\n",
    "    print(len(result1))\n",
    "    itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result1, pseudo_labels1)\n",
    "    \n",
    "    if i<2:\n",
    "        augmented_data = []\n",
    "        augmented_labels = []\n",
    "        for idx in result1:\n",
    "            input_data = data[idx]  # Get the data for the current index\n",
    "            input_data = Image.fromarray(input_data)\n",
    "            for _ in range(dmp):  # Perform 5 augmentations\n",
    "                augmented_input = augmentation_transforms(input_data)  # Apply augmentations\n",
    "                augmented_input = (augmented_input.permute(1, 2, 0).numpy()*255).astype(np.uint8)\n",
    "                augmented_data.append(augmented_input)\n",
    "                augmented_labels.append(itr1_pseudo[idx])  # Use the pseudo-label for augmented data\n",
    "\n",
    "        augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "        # Concatenate the original data and augmented data\n",
    "        data = np.concatenate((data, np.array(augmented_data)), axis=0)\n",
    "        itr1_pseudo = np.concatenate((itr1_pseudo, augmented_labels), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset = NDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True,num_workers=4)\n",
    "    datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloaderpred = DataLoader(datasetpred, batch_size=bsize, shuffle=False,num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "    val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    # model.train()\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "    if(i==1):\n",
    "        optimizer = optim.Adam(model.parameters(), 0.000001)\n",
    "\n",
    "net1=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data = data1\n",
    "# # new_labels = new_labels1\n",
    "# val_data = test_images\n",
    "# val_labels = test_labels\n",
    "# val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "# model.eval()\n",
    "# all_predictions = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in val_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "# # model.train()\n",
    "# from sklearn.metrics import f1_score\n",
    "# f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "# print(f'Fold {fold + 1} F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1555\n",
      "Epoch [2/5], Loss: 0.1396\n",
      "Epoch [3/5], Loss: 0.1369\n",
      "Epoch [4/5], Loss: 0.1371\n",
      "Epoch [5/5], Loss: 0.1327\n",
      "Fold 1 F1 Score: 0.2348\n",
      "Epoch [1/4], Loss: 0.1341\n",
      "Epoch [2/4], Loss: 0.1320\n",
      "Epoch [3/4], Loss: 0.1331\n",
      "Epoch [4/4], Loss: 0.1308\n",
      "Fold 1 F1 Score: 0.2523\n",
      "2420\n",
      "Iteration: 0\n",
      "Epoch [1/7], Loss: 0.0822\n",
      "Epoch [2/7], Loss: 0.0736\n",
      "Epoch [3/7], Loss: 0.0724\n",
      "Epoch [4/7], Loss: 0.0702\n",
      "Epoch [5/7], Loss: 0.0697\n",
      "Epoch [6/7], Loss: 0.0696\n",
      "Epoch [7/7], Loss: 0.0683\n",
      "7219\n",
      "Fold 1 F1 Score: 0.2525\n",
      "Iteration: 1\n",
      "Epoch [1/7], Loss: 0.0442\n",
      "Epoch [2/7], Loss: 0.0428\n",
      "Epoch [3/7], Loss: 0.0424\n",
      "Epoch [4/7], Loss: 0.0410\n",
      "Epoch [5/7], Loss: 0.0413\n",
      "Epoch [6/7], Loss: 0.0404\n",
      "Epoch [7/7], Loss: 0.0404\n",
      "21637\n",
      "Fold 1 F1 Score: 0.2490\n",
      "Iteration: 2\n",
      "Epoch [1/7], Loss: 0.0288\n",
      "Epoch [2/7], Loss: 0.0279\n",
      "Epoch [3/7], Loss: 0.0278\n",
      "Epoch [4/7], Loss: 0.0275\n",
      "Epoch [5/7], Loss: 0.0272\n",
      "Epoch [6/7], Loss: 0.0271\n",
      "Epoch [7/7], Loss: 0.0270\n",
      "64826\n",
      "Fold 1 F1 Score: 0.2471\n"
     ]
    }
   ],
   "source": [
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(data1)):\n",
    "from PreResNet import *\n",
    "\n",
    "def create_model():\n",
    "    model = ResNet18(num_classes=4)\n",
    "    model = model.cuda()\n",
    "    return model\n",
    "#     data = data1[train_idx]\n",
    "#     new_labels = new_labels1[train_idx]\n",
    "#     val_data = data1[val_idx]\n",
    "#     val_labels = targets[val_idx]\n",
    "fold = 0\n",
    "\n",
    "data = data1\n",
    "new_labels = new_labels1\n",
    "val_data = val_images\n",
    "val_labels = vallabels\n",
    "\n",
    "#     model = models.resnet50(pretrained=True)  \n",
    "# # model.load_state_dict(torch.load('./2024_08_14-05_29_56_PM/best_cp/modelk.pth',map_location='cuda:0'),strict=False)\n",
    "\n",
    "\n",
    "#     model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# model.to(device)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# criterion1 = nn.CrossEntropyLoss(reduction='none',label_smoothing=0.1)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Model, Loss, and Optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# resnet = models.resnet18(pretrained=False)\n",
    "resnet = create_model()\n",
    "# resnet.load_state_dict(torch.load('net2chest0.2withoutssl.pth'))\n",
    "resnet.load_state_dict(torch.load('net2HAM0.2.pth'))\n",
    "\n",
    "num_ftrs = resnet.linear.in_features\n",
    "resnet.linear = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "model = nn.Sequential(\n",
    "    resnet,                    # ResNet-50 backbone without classification layer\n",
    "    nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "    nn.ReLU(),                  # ReLU activation function\n",
    "    nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    ")\n",
    "# model=resnet\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# num_ftrs = resnet.fc.in_features\n",
    "# resnet.fc = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "# model = nn.Sequential(\n",
    "#     resnet,                    # ResNet-50 backbone without classification layer\n",
    "#     nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "#     nn.ReLU(),                  # ReLU activation function\n",
    "#     nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.resnet50(pretrained=False)\n",
    "# model.load_state_dict(torch.load('resnet50_weights.pth'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "criterion = NTXentLoss(temperature=0.3)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "dataset = ExampleDataset1(data, new_labels,transform=SimCLRTransform)\n",
    "dataloader = DataLoader(dataset, batch_size=400, shuffle=True,num_workers=4)\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        img_1, img_2 = images  # Two augmented views of the same image\n",
    "\n",
    "        # Move to device\n",
    "        img_1, img_2 = img_1.to(device), img_2.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        z_i = model(img_1)\n",
    "        z_j = model(img_2)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(z_i, z_j)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    \n",
    "    \n",
    "dataset = ExampleDataset1(data, new_labels,transform=SimCLRTransform1)\n",
    "dataloader = DataLoader(dataset, batch_size=200, shuffle=True,num_workers=4)\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        img_1, img_2 = images  # Two augmented views of the same image\n",
    "\n",
    "        # Move to device\n",
    "        img_1, img_2 = img_1.to(device), img_2.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        z_i = model(img_1)\n",
    "        z_j = model(img_2)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(z_i, z_j)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "torch.save(model.state_dict(), 'simclrsslvarybsdividemix1.pth')\n",
    "'''\n",
    "\n",
    "# model.load_state_dict(torch.load('simclrssl1.pth'))\n",
    "# model.to(device)\n",
    "# criterion = NTXentLoss(temperature=0.5)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "# epochs = 20\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, _ in dataloader:\n",
    "#         img_1, img_2 = images  # Two augmented views of the same image\n",
    "\n",
    "#         # Move to device\n",
    "#         img_1, img_2 = img_1.to(device), img_2.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         z_i = model(img_1)\n",
    "#         z_j = model(img_2)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(z_i, z_j)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "# torch.save(model.state_dict(), 'simclrssladamsgd.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# transform = BYOLTransform()\n",
    "# dataset = CIFAR10BYOLDataset(data, new_labels,transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True)\n",
    "\n",
    "# Define BYOL model and optimizer\n",
    "# base_encoder = models.resnet50(pretrained=False)\n",
    "# base_encoder.load_state_dict(torch.load('resnet50_weights.pth'))\n",
    "# model = BYOL(base_encoder).to('cuda')\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Train for a number of epochs\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train_byol(dataloader, model, optimizer)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict(), 'byolssl.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load('simclrsslvarybs.pth'))\n",
    "# model.load_state_dict(torch.load('simclrsslvarybsdividemix1.pth'))\n",
    "# model.load_state_dict(torch.load('byol.pth'))\n",
    "\n",
    "# model.to(device)\n",
    "# model.fc = nn.Sequential(\n",
    "# #     nn.Dropout(0.3),  # Dropout with probability 0.3\n",
    "#     nn.Linear(model.fc.in_features, 10)  # The output layer with 10 classes (for CIFAR-10)\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "#simclr\n",
    "model = nn.Sequential(\n",
    "    model,                    # ResNet-50 backbone without classification layer\n",
    "    nn.ReLU(),                  # ReLU activation function\n",
    "    nn.Dropout(0.7),\n",
    "    nn.Linear(128, 4)         # Second linear layer projecting to 128 dimensions\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sslafterdividemix\n",
    "# num_ftrs = model.linear.in_features\n",
    "# model.linear = nn.Identity()\n",
    "# model = nn.Sequential(\n",
    "#     model,                    # ResNet-50 backbone without classification layer\n",
    "# #     nn.ReLU(),                  # ReLU activation function\n",
    "# #     nn.Dropout(0.7),\n",
    "#     nn.Linear(num_ftrs, 10)         # Second linear layer projecting to 128 dimensions\n",
    "# )\n",
    "# simclr end\n",
    "\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load('byolssl.pth'))\n",
    "\n",
    "\n",
    "# pretrained_backbone = model.online_encoder\n",
    "\n",
    "# Creating a fine-tuned model for CIFAR-10 classification\n",
    "# model = FineTunedModel(pretrained_backbone).to('cuda')\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     model,                    # ResNet-50 backbone without classification layer\n",
    "#     nn.ReLU(),                  # ReLU activation function\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.Linear(256, 10)         # Second linear layer projecting to 128 dimensions\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "criterion1 = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.95,weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "criterion1 = FocalLoss(alpha=0.25, gamma=2.0,reduction='none')\n",
    "\n",
    "dataset = NDataset(data, new_labels,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True,num_workers=4)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     dataset.update_confidence(model, device)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets, confidences in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets=targets.to(device) \n",
    "        confidences=confidences.to(device)\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs = model(inputs)  \n",
    "        loss = criterion(outputs, targets)\n",
    "# #         confidences = torch.softmax(outputs, dim=1)\n",
    "#         loss = training_criterion(outputs, targets, confidences)\n",
    "\n",
    "        loss.backward() \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()  \n",
    "        # scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "\n",
    "\n",
    "val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# model.train()\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.95)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "stages = [2,3,4]\n",
    "# stages=[2,4,7,9,11]\n",
    "datasetpred = ExampleDataset(data, new_labels,transform=transform)\n",
    "dataloaderpred = DataLoader(datasetpred, batch_size=bsize, shuffle=False,num_workers=4)\n",
    "pseudo_labels=[]\n",
    "confident_labels=[]\n",
    "last_epoch=stages[-1]\n",
    "ep=0;\n",
    "for st in stages:\n",
    "    model.train()\n",
    "    num_epochs = st;\n",
    "    for epoch in range(ep,num_epochs):\n",
    "#         dataset.update_confidence(model, device)\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets, confidences in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets=targets.to(device) \n",
    "            confidences=confidences.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute loss\n",
    "# #             confidences = torch.softmax(outputs, dim=1)\n",
    "#             loss = training_criterion(outputs, targets, confidences)\n",
    "        \n",
    "            loss.backward()  # Backward pass\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()  # Optimize\n",
    "            # scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "    all_predictions = []\n",
    "    reliable_indice=[]\n",
    "    model.eval()\n",
    "#     pred, reliable_indices = pseudo_labeling(model, dataloaderpred, criterion1, device)\n",
    "#     pseudo_labels.append(pred)\n",
    "#     confident_labels.append(reliable_indices)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaderpred: \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Calculate per-sample loss\n",
    "            loss_per_sample = criterion1(outputs, labels)\n",
    "            \n",
    "\n",
    "\n",
    "            # print(loss_per_sample)\n",
    "            reliable_indices = loss_per_sample < 0.25  # Example threshold for loss\n",
    "            # print(reliable_indices)\n",
    "            # Extend reliable indices list with boolean values indicating reliability\n",
    "            reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "\n",
    "    pseudo_labels.append(np.array(all_predictions))\n",
    "    confident_labels.append(np.array(reliable_indice))\n",
    "    ep=st;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# model.train()\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "result = common_elements_same_index(confident_labels)\n",
    "print(len(result))\n",
    "itr1_pseudo=new_labels\n",
    "itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result, pseudo_labels)\n",
    "\n",
    "augmented_data = []\n",
    "augmented_labels = []\n",
    "for idx in result:\n",
    "    input_data = data[idx]  # Get the data for the current index\n",
    "    input_data = Image.fromarray(input_data)\n",
    "    for _ in range(dmp):  # Perform 5 augmentations\n",
    "        augmented_input = augmentation_transforms(input_data)  # Apply augmentations\n",
    "        augmented_input = (augmented_input.permute(1, 2, 0).numpy()*255).astype(np.uint8)\n",
    "        augmented_data.append(augmented_input)\n",
    "        augmented_labels.append(itr1_pseudo[idx])  # Use the pseudo-label for augmented data\n",
    "\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Concatenate the original data and augmented data\n",
    "data = np.concatenate((data, np.array(augmented_data)), axis=0)\n",
    "itr1_pseudo = np.concatenate((itr1_pseudo, augmented_labels), axis=0)\n",
    "\n",
    "\n",
    "dataset = NDataset(data, itr1_pseudo,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True,num_workers=4)\n",
    "datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "dataloaderpred = DataLoader(datasetpred, batch_size=bsize, shuffle=False,num_workers=4)\n",
    "p=[]\n",
    "a=[]\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "augmented_epochs=[1,3,5]\n",
    "for i in range(3):\n",
    "    print(f'Iteration: {i}')\n",
    "    stages = [2,5,7]\n",
    "#     stages = [3,5,6,8,10]\n",
    "    pseudo_labels1=[]\n",
    "    confident_labels=[]\n",
    "    last_epoch=stages[-1]\n",
    "    ep=0;\n",
    "    for st in stages:\n",
    "        model.train()\n",
    "        num_epochs = st;\n",
    "        for epoch in range(ep,num_epochs):\n",
    "#             dataset.update_confidence(model, device)\n",
    "            running_loss = 0.0\n",
    "            for inputs, targets, confidences in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets=targets.to(device) \n",
    "                confidences=confidences.to(device)\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "                loss = criterion(outputs, targets)  # Compute loss\n",
    "# #                 confidences = torch.softmax(outputs, dim=1)\n",
    "#                 loss = training_criterion(outputs, targets, confidences)\n",
    "\n",
    "                loss.backward()  # Backward pass\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()  # Optimize\n",
    "                # scheduler.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "        all_predictions = []\n",
    "        reliable_indice=[]\n",
    "        model.eval()\n",
    "#         pred, reliable_indices = pseudo_labeling(model, dataloaderpred, criterion1, device)\n",
    "#         pseudo_labels1.append(pred)\n",
    "#         confident_labels.append(reliable_indices)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaderpred: \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass to get outputs\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Get the predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                # Calculate per-sample loss\n",
    "                loss_per_sample = criterion1(outputs, labels)\n",
    "            \n",
    "\n",
    "                # Set a loss threshold (lower loss means more reliable)\n",
    "                reliable_indices = loss_per_sample < 0.25  # Example threshold for loss\n",
    "\n",
    "                # Extend reliable indices list with boolean values indicating reliability\n",
    "                reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "        pseudo_labels1.append(np.array(all_predictions))\n",
    "        confident_labels.append(np.array(reliable_indice))\n",
    "        ep=st;\n",
    "    result1 = common_elements_same_index(confident_labels)\n",
    "    p.append(result1)\n",
    "    a.append(pseudo_labels1)\n",
    "    print(len(result1))\n",
    "    itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result1, pseudo_labels1)\n",
    "    \n",
    "    if i <2:\n",
    "        augmented_data = []\n",
    "        augmented_labels = []\n",
    "        for idx in result1:\n",
    "            input_data = data[idx]  # Get the data for the current index\n",
    "            input_data = Image.fromarray(input_data)\n",
    "            for _ in range(dmp):  # Perform 5 augmentations\n",
    "                augmented_input = augmentation_transforms(input_data)  # Apply augmentations\n",
    "                augmented_input = (augmented_input.permute(1, 2, 0).numpy()*255).astype(np.uint8)\n",
    "                augmented_data.append(augmented_input)\n",
    "                augmented_labels.append(itr1_pseudo[idx])  # Use the pseudo-label for augmented data\n",
    "\n",
    "        augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "        # Concatenate the original data and augmented data\n",
    "        data = np.concatenate((data, np.array(augmented_data)), axis=0)\n",
    "        itr1_pseudo = np.concatenate((itr1_pseudo, augmented_labels), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset = NDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True,num_workers=4)\n",
    "    datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloaderpred = DataLoader(datasetpred, batch_size=bsize, shuffle=False,num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "    val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    # model.train()\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "    if(i==1):\n",
    "        optimizer = optim.Adam(model.parameters(), 0.000001)\n",
    "\n",
    "'''    \n",
    "augmented_epochs=[1,2,4]\n",
    "for i in range(0):\n",
    "    print(f'Iteration: {2+i}')\n",
    "    stages = [3,6,10]\n",
    "    pseudo_labels1=[]\n",
    "    confident_labels=[]\n",
    "    last_epoch=stages[-1]\n",
    "    ep=0;\n",
    "    for st in stages:\n",
    "        model.train()\n",
    "        num_epochs = st;\n",
    "        for epoch in range(ep,num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, targets in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets=targets.to(device) \n",
    "                confidences=confidences.to(device)\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "                loss = criterion(outputs, targets)  # Compute loss\n",
    "\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Optimize\n",
    "                # scheduler.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "        all_predictions = []\n",
    "        reliable_indice=[]\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaderpred: \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass to get outputs\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Get the predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                # Calculate per-sample loss\n",
    "                loss_per_sample = criterion1(outputs, labels) \n",
    "\n",
    "                # Set a loss threshold (lower loss means more reliable)\n",
    "                reliable_indices = loss_per_sample < 0.25  # Example threshold for loss\n",
    "\n",
    "                # Extend reliable indices list with boolean values indicating reliability\n",
    "                reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "        pseudo_labels1.append(np.array(all_predictions))\n",
    "        confident_labels.append(np.array(reliable_indice))\n",
    "        ep=st;\n",
    "    result1 = common_elements_same_index(confident_labels)\n",
    "    p.append(result1)\n",
    "    a.append(pseudo_labels1)\n",
    "    print(len(result1))\n",
    "    itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result1, pseudo_labels1)\n",
    "    \n",
    "#     if i in augmented_epochs:\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "    for idx in result1:\n",
    "        input_data = data[idx]  # Get the data for the current index\n",
    "        input_data = Image.fromarray(input_data)\n",
    "        for _ in range(dmp):  # Perform 5 augmentations\n",
    "            augmented_input = augmentation_transforms(input_data)  # Apply augmentations\n",
    "            augmented_input = (augmented_input.permute(1, 2, 0).numpy()*255).astype(np.uint8)\n",
    "            augmented_data.append(augmented_input)\n",
    "            augmented_labels.append(itr1_pseudo[idx])  # Use the pseudo-label for augmented data\n",
    "\n",
    "    augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "    # Concatenate the original data and augmented data\n",
    "    data = np.concatenate((data, np.array(augmented_data)), axis=0)\n",
    "    itr1_pseudo = np.concatenate((itr1_pseudo, augmented_labels), axis=0)\n",
    "    \n",
    "    \n",
    "    dataset = NDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True,num_workers=4)\n",
    "    datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "    dataloaderpred = DataLoader(datasetpred, batch_size=bsize, shuffle=False,num_workers=4)\n",
    "\n",
    "\n",
    "    val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    # model.train()\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "'''\n",
    "net2=model\n",
    "    \n",
    "# for i in range(1):\n",
    "#     print(f'Iteration: {2+i}')\n",
    "#     stages = [2,5]\n",
    "#     pseudo_labels1=[]\n",
    "#     confident_labels=[]\n",
    "#     last_epoch=stages[-1]\n",
    "#     ep=0;\n",
    "#     for st in stages:\n",
    "#         model.train()\n",
    "#         num_epochs = st;\n",
    "#         for epoch in range(ep,num_epochs):\n",
    "#             running_loss = 0.0\n",
    "#             for inputs, targets in dataloader:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 targets=targets.to(device) \n",
    "#                 optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "#                 outputs = model(inputs)  # Forward pass\n",
    "#                 loss = criterion(outputs, targets)  # Compute loss\n",
    "\n",
    "#                 loss.backward()  # Backward pass\n",
    "#                 optimizer.step()  # Optimize\n",
    "#                 # scheduler.step()\n",
    "#                 running_loss += loss.item()\n",
    "\n",
    "#             print(f'Epoch [{epoch + 1}/{last_epoch}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "#         all_predictions = []\n",
    "#         reliable_indice=[]\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in dataloaderpred: \n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 # Forward pass to get outputs\n",
    "#                 outputs = model(inputs)\n",
    "\n",
    "#                 # Get the predictions\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "#                 # Calculate per-sample loss\n",
    "#                 loss_per_sample = criterion1(outputs, labels) \n",
    "\n",
    "#                 # Set a loss threshold (lower loss means more reliable)\n",
    "#                 reliable_indices = loss_per_sample < 0.25  # Example threshold for loss\n",
    "\n",
    "#                 # Extend reliable indices list with boolean values indicating reliability\n",
    "#                 reliable_indice.extend(reliable_indices.cpu().numpy())\n",
    "\n",
    "#         pseudo_labels1.append(np.array(all_predictions))\n",
    "#         confident_labels.append(np.array(reliable_indice))\n",
    "#         ep=st;\n",
    "#     result1 = common_elements_same_index(confident_labels)\n",
    "#     p.append(result1)\n",
    "#     a.append(pseudo_labels1)\n",
    "#     print(len(result1))\n",
    "#     itr1_pseudo = replace_elements_at_indices(itr1_pseudo, result1, pseudo_labels1)\n",
    "#     dataset = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "#     dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True)\n",
    "#     datasetpred = ExampleDataset(data, itr1_pseudo,transform=transform)\n",
    "#     dataloaderpred = DataLoader(dataset, batch_size=bsize, shuffle=False)\n",
    "\n",
    "\n",
    "#     val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "#     model.eval()\n",
    "#     all_predictions = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             all_predictions.extend(predicted.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#     # model.train()\n",
    "#     from sklearn.metrics import f1_score\n",
    "#     f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "#     print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n",
    "\n",
    "# val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "# model.eval()\n",
    "# all_predictions = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in val_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "# # model.train()\n",
    "# from sklearn.metrics import f1_score\n",
    "# f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "# print(f'Fold {fold + 1} F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1 Score: 0.2242\n"
     ]
    }
   ],
   "source": [
    "data = data1\n",
    "new_labels = new_labels1\n",
    "val_data = test_images\n",
    "val_labels = test_labels\n",
    "val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs1 = net1(inputs)\n",
    "        outputs2 = net2(inputs)\n",
    "        outputs = (outputs1 + outputs2) / 2\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "# model.train()\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f'Fold {fold + 1} F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data1\n",
    "# new_labels = new_labels1\n",
    "# val_data = test_images\n",
    "# val_labels = test_labels\n",
    "# val_dataset = ExampleDataset(val_data, val_labels,transform=transform)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False)\n",
    "# model.eval()\n",
    "# all_predictions = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in val_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "# # model.train()\n",
    "# from sklearn.metrics import f1_score\n",
    "# f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "# print(f'Fold {fold + 1} F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def common_elements_same_index(arrays):\n",
    "#     if not arrays:\n",
    "#         return []\n",
    "\n",
    "#     # Number of arrays\n",
    "#     num_arrays = len(arrays)\n",
    "    \n",
    "#     # Length of the first array\n",
    "#     num_elements = len(arrays[0])\n",
    "\n",
    "#     # Store indices where all arrays have the same element\n",
    "#     common_indices = []\n",
    "    \n",
    "#     # Iterate over each index\n",
    "#     for i in range(num_elements):\n",
    "#         # Get the element at index i in the first array\n",
    "#         element = arrays[0][i]\n",
    "        \n",
    "#         # Check if all arrays have the same element at index i\n",
    "#         if all(array[i] == element for array in arrays):\n",
    "#             common_indices.append(i)\n",
    "    \n",
    "#     return common_indices\n",
    "\n",
    "\n",
    "# result1 = common_elements_same_index(pseudo_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2083934,
     "status": "ok",
     "timestamp": 1723895235061,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "rKvtkfHa-oho",
    "outputId": "68e43e1e-db07-4ddb-da59-37ae73dee515"
   },
   "outputs": [],
   "source": [
    "# # num_epochs = 5\n",
    "# running_loss=50;\n",
    "# epoch=0;\n",
    "# while True:\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, targets in dataloader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         targets=targets.to(device) \n",
    "#         optimizer.zero_grad()  \n",
    "\n",
    "#         outputs = model(inputs) \n",
    "#         loss = criterion(outputs, targets)  \n",
    "\n",
    "#         loss.backward() \n",
    "#         optimizer.step() \n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "# #   print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "#     print(f'Epoch [{epoch + 1}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "#     if running_loss / len(dataloader)<0.6:\n",
    "#         break;\n",
    "#     epoch=epoch+1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "UP2edhhu-zPp"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# torch.save(model.state_dict(), 'resnet50_weights_cifar10_wbyol_loss.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35116,
     "status": "ok",
     "timestamp": 1723906830244,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "z6cZJNqBLU4h",
    "outputId": "1b13cb2e-baef-4fc4-acb4-fd25807b9254"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.datasets import MNIST\n",
    "# from torchvision import models\n",
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "# resnet = models.resnet50(pretrained=False)\n",
    "# # resnet.load_state_dict(torch.load('resnet50_weights.pth'))\n",
    "\n",
    "# # Remove the classification layer (fully connected layer)\n",
    "# num_ftrs = resnet.fc.in_features\n",
    "# resnet.fc = nn.Identity()  # Replace the FC layer with an Identity layer\n",
    "\n",
    "# # Define the projection head (2-layer MLP) and include it with the ResNet model\n",
    "# model = nn.Sequential(\n",
    "#     resnet,                    # ResNet-50 backbone without classification layer\n",
    "#     nn.Linear(num_ftrs, 512),   # First linear layer of the projection head\n",
    "#     nn.ReLU(),                  # ReLU activation function\n",
    "#     nn.Linear(512, 128)         # Second linear layer projecting to 128 dimensions\n",
    "# )\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     model,                    # ResNet-50 backbone without classification layer\n",
    "#     nn.ReLU(),                  # ReLU activation function\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.Linear(128, 10)         # Second linear layer projecting to 128 dimensions\n",
    "# )\n",
    "\n",
    "# # model = models.resnet50(pretrained=True)\n",
    "# # model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# # model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# model.load_state_dict(torch.load('resnet50_weights_cifar10_wbyol_loss.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# # transform = transforms.Compose([\n",
    "# # #     transforms.ToPILImage(),  \n",
    "# #     transforms.ToTensor(),\n",
    "# #     transforms.Normalize((0.467, 0.459, 0.519), (0.207, 0.233, 0.261))\n",
    "# # ])\n",
    "\n",
    "# test_images = np.load('data/cifar10/test_images.npy')\n",
    "# test_labels = np.load('data/cifar10/test_labels.npy')\n",
    "\n",
    "# # test_dataset = ExampleDataset(test_images, test_labels,transform)\n",
    "# test_images=test_images.reshape(-1, 3, 32, 32)\n",
    "# data=test_images\n",
    "# data=data.reshape(-1, 3, 32, 32)\n",
    "# data=data.transpose((0, 2, 3, 1))\n",
    "# testl = torch.from_numpy(test_labels)\n",
    "# test_dataset=ExampleDataset(data,testl,transform=transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=bsize, shuffle=False)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# all_predictions = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in test_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels=labels.to(device) \n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# print(np.array(all_predictions[:1000])-np.array(testl[:1000]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1723911251111,
     "user": {
      "displayName": "Anuj Gupta",
      "userId": "07995943728544839196"
     },
     "user_tz": -330
    },
    "id": "nrBmCCe2ts62",
    "outputId": "07c5682d-dd4e-46d9-9ea1-891aa3d34e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87  0 55  0]\n",
      " [29  0 97  0]\n",
      " [57  0 81  0]\n",
      " [44  0 22  0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(np.array(all_labels), np.array(all_predictions))\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22422335154904421\n"
     ]
    }
   ],
   "source": [
    "# from torchmetrics.classification import MulticlassF1Score\n",
    "# f1_score = MulticlassF1Score(num_classes=10, average='macro')\n",
    "\n",
    "# all_predictions = torch.tensor(all_predictions)\n",
    "# all_labels = torch.tensor(all_labels)\n",
    "\n",
    "# f1_score.update(all_predictions, all_labels)\n",
    "# f1 = f1_score.compute()\n",
    "\n",
    "# print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f1)\n",
    "# print(f'Fold {fold + 1} F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "Accuracy: 0.2999\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torch\n",
    "\n",
    "# Assuming all_predictions and all_labels are defined as lists or arrays\n",
    "all_predictions = torch.tensor(all_predictions)\n",
    "all_labels = torch.tensor(all_labels)\n",
    "\n",
    "# Initialize accuracy metric for multiclass classification\n",
    "accuracy_metric = MulticlassAccuracy(num_classes=10)\n",
    "\n",
    "# Update metric with predictions and labels\n",
    "accuracy_metric.update(all_predictions, all_labels)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = accuracy_metric.compute()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "T0QmFwdP4pTN"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from torchvision import transforms, datasets\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import torchvision.models as models\n",
    "# import torch.nn as nn\n",
    "\n",
    "# model = models.resnet50(pretrained=False)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# model.load_state_dict(torch.load('resnet50_weights_cifar10.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# # transform = transforms.Compose([\n",
    "# # #     transforms.ToPILImage(),  \n",
    "# #     transforms.ToTensor(),\n",
    "# #     transforms.Normalize((0.467, 0.459, 0.519), (0.207, 0.233, 0.261))\n",
    "# # ])\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data, transform=None):\n",
    "#         self.data = data\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         x = self.data[index]\n",
    "#         if self.transform:\n",
    "#             x = self.transform(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# original_images = np.load('data/cifar10/train_images.npy')\n",
    "# data=original_images\n",
    "# data=data.reshape(-1, 3, 32, 32)\n",
    "# data=data.transpose((0, 2, 3, 1))\n",
    "\n",
    "# dataset = CustomDataset(images, transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=bsize, shuffle=False)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# all_predictions = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs in dataloader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# pseudo_labels = np.array(all_predictions)\n",
    "\n",
    "# np.save('pseudo_labels.npy', pseudo_labels)\n",
    "\n",
    "# print(f'Pseudo labels have been saved to pseudo_labels.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_images = np.load('data/cifar10/train_images.npy')\n",
    "# pseudo_labels = np.load('pseudo_labels.npy')\n",
    "# data=original_images\n",
    "# data=data.reshape(-1, 3, 32, 32)\n",
    "# data=data.transpose((0, 2, 3, 1))\n",
    "# targets = torch.from_numpy(pseudo_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ExampleDataset(data, targets,transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=bsize, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet50(pretrained=False)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 5\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, targets in dataloader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         targets=targets.to(device) \n",
    "#         optimizer.zero_grad() \n",
    "\n",
    "#         outputs = model(inputs)  \n",
    "#         loss = criterion(outputs, targets)  \n",
    "\n",
    "#         loss.backward() \n",
    "#         optimizer.step()  \n",
    "#         # scheduler.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'resnet50_weights_cifar10.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.datasets import MNIST\n",
    "# from torchvision import models\n",
    "# import torch.nn as nn\n",
    "\n",
    "# model = models.resnet50(pretrained=False)\n",
    "# model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# model.load_state_dict(torch.load('resnet50_weights_cifar10.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# # transform = transforms.Compose([\n",
    "# # #     transforms.ToPILImage(),  \n",
    "# #     transforms.ToTensor(),\n",
    "# #     transforms.Normalize((0.467, 0.459, 0.519), (0.207, 0.233, 0.261))\n",
    "# # ])\n",
    "\n",
    "# test_images = np.load('data/cifar10/test_images.npy')\n",
    "# test_labels = np.load('data/cifar10/test_labels.npy')\n",
    "\n",
    "# test_images=test_images.reshape(-1, 3, 32, 32)\n",
    "# data=test_images\n",
    "# data=data.reshape(-1, 3, 32, 32)\n",
    "# data=data.transpose((0, 2, 3, 1))\n",
    "# testl = torch.from_numpy(test_labels)\n",
    "# test_dataset=ExampleDataset(data,testl,transform=transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=bsize, shuffle=False)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# all_predictions = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in test_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels=labels.to(device) \n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# print(np.array(all_predictions[:1000])-np.array(testl[:1000]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(np.array(all_labels), np.array(all_predictions))\n",
    "\n",
    "# print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchmetrics.classification import MulticlassF1Score\n",
    "# f1_score = MulticlassF1Score(num_classes=10, average='macro')\n",
    "\n",
    "# all_predictions = torch.tensor(all_predictions)\n",
    "# all_labels = torch.tensor(all_labels)\n",
    "\n",
    "# f1_score.update(all_predictions, all_labels)\n",
    "# f1 = f1_score.compute()\n",
    "\n",
    "# print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOk4guyS0I0FTG3Yna/p2Aa",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
